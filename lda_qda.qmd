# Discriminant analysis

[Discriminant factor analysis]{style="color:blue"} (DA) or simply [discriminant analysis]{style="color:blue"} is a statistical technique that aims to:

-   describe

-   explain

-   predict

the membership to predefined groups (classes, modalities of the variable to be predicted...) of a set of observations (individuals, examples...) from a series of predictive variables (descriptors, exogenous variables...).

In [medicine]{style="color:blue"}, to detect high-risk cardiac groups based on characteristics such as diet, smoking or not, family history, ...

Discriminant analysis can be a descriptive technique:

-   The objective is to propose a [new representation system, latent variables]{style="color:blue"} formed from linear combinations of predictive variables, which make it possible to discern groups of individuals as much as possible.

Discriminant analysis can be a predictive technique:

-   It involves building a [classification function (assignment rule)]{style="color:blue"} that predicts the group to which an individual belongs based on the values taken by the predictive variables.

## Principle

We have a sample of $n$ observations distributed in $K$ groups of sizes $n_k$. The number of classes $K$ is fixed in advance.

```{=tex}
\begin{aligned}
X_n = 
\begin{bmatrix}
    x_{11} & x_{12} & x_{13} & \dots  & x_{1p} \\
    x_{21} & x_{22} & x_{23} & \dots  & x_{2p} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    x_{n1} & x_{n2} & x_{n3} & \dots  & x_{np}
\end{bmatrix}

\quad
,
\quad

Y_n = 
\begin{bmatrix}
    y_{1} \\
    y_{2} \\
    \vdots \\
    y_{n}
\end{bmatrix}\\

y \in \{1,...,K\}

\end{aligned}
```
In discriminant analysis, we assume that the [conditional distribution of X to class Y]{style="color:blue"} is **parametric** and **Gaussian** $(X|Y = k \sim \mathcal{N}(\mu_{K},\Sigma_{K}))$

Where:

-   $\mu_{k}$: the centers of gravity of the conditional point clouds

-   $\Sigma_k$: their variance-covariance matrix

::: callout-note
Then the **probability density function** (PDF) is:

```{=tex}
\begin{aligned}

f(x|Y=k) = \frac{1}{(2 \pi)^\frac{p}{2}|\Sigma_{k}|^{\frac{1}{2}}}exp\{ -\frac{1}{2}(x-\mu_k)^\intercal \sum\nolimits_{k}^{-1}(x-\mu_k)\}

\end{aligned}
```
:::

::: {layout-ncol="2"}
![Linear discriminant analysis](fig/classification/lda.png){width="500" fig-align="center"}

![Quadratic discriminant analysis](fig/classification/qda.png){width="500" fig-align="center"}
:::

In training step, we want to find the classification rule $g(x)$ that maximize the $\mathbb{P}(Y = k | X = x)$. The Bayesian rule write as:

```{=tex}
\begin{aligned}

g(x) &= \mathbb{P}(Y = k | X = x) \\
     
     &= f(x|Y=k) \mathbb{P}(Y=k)\\
     
     &= log(f(x|Y=k)) + log(\pi_{k})\\

\end{aligned}
```
Maximum likelihood estimators

```{=tex}
\begin{aligned}

\widehat{\pi} &= \frac{n_k}{n} &  \widehat{\mu_k} = \frac{1}{n_k}\sum_{i;y_i = k }^{}x_i\\
     
    \widehat{\Sigma_k} &= \frac{1}{n_k}\sum_{i;y_i = k }^{}(x_i-\widehat{\mu_k})^\intercal (x_i-\widehat{\mu_k})\\

\end{aligned}
```
## Linear Discriminant analysis (LDA)

We assume that K clusters have the same $\Sigma$, $\Sigma_k = \Sigma$ for k = 1,··· ,K. Now the likelihood is

```{=tex}
\begin{aligned}

f(x|Y=k) = \frac{1}{(2 \pi)^\frac{p}{2}|\Sigma|^{\frac{1}{2}}}exp\{ -\frac{1}{2}(x-\mu_k)^\intercal \sum\nolimits_{}^{-1}(x-\mu_k)\}

\end{aligned}
```
take the log

```{=tex}
\begin{aligned}

log(f(x|Y=k)) &= \underbrace{- \frac{\pi}{2} log(2 \pi) - \frac{1}{2}log|\Sigma|} - \frac{1}{2}(x-\mu_k)^\intercal \sum\nolimits_{}^{-1}(x-\mu_k) \\

&= independent\ on\ k \\

&= - \frac{1}{2}(x-\mu_k)^\intercal \sum\nolimits_{}^{-1}(x-\mu_k)  + constants \\

&= - \frac{1}{2}(x^\intercal \Sigma^{-1}x -2x^\intercal \Sigma^{-1}\mu_k+\mu^{\intercal}_k \sum\nolimits_{}^{-1}\mu_k) + constants \\

&=  \underbrace{-\frac{1}{2}x^\intercal \Sigma^{-1}x} + x^\intercal \Sigma^{-1}\mu_k -\frac{1}{2} \mu^{\intercal}_k \sum\nolimits_{}^{-1}\mu_k + constants \\

&= same\ for\ all\ k

\end{aligned}
```
We want to maximize $g(x) = \mathbb{P}(Y = k | X = x) = log(f(x|Y=k)) + log(\pi_{k})$. So $L_{k}(x)= x^\intercal \Sigma^{-1}\mu_k -\frac{1}{2} \mu^{\intercal}_k \sum\nolimits_{}^{-1}\mu_k + log(\pi_{k})$ is called [linear discriminant function]{style="color:blue"}.

The posterior probabilities of the classes are calculated as follows:

```{=tex}
\begin{aligned}

\mathbb{P}(Y = k | X = x) = \frac{exp(L_k(x))}{\sum_{\ell=1}^{k}exp(L_\ell(x))}

\end{aligned}
```
## Quadratic Discriminant analysis (QDA)

We no longer assume that K clusters have the same $\Sigma$, $\Sigma_k = \Sigma$ for k = 1,··· ,K. Take the log likelihood of PDF of multivariate Gaussian distribution

```{=tex}
\begin{aligned}

log(f(x|Y=k)) &= \underbrace{- \frac{\pi}{2} log(2 \pi)} - \frac{1}{2}log|\Sigma_k|^{-1} - \frac{1}{2}(x-\mu_k)^\intercal \sum\nolimits_{k}^{-1}(x-\mu_k) \\

&= independent\ on\ k \\

&= - \frac{1}{2}log|\Sigma_k|^{-1} - \frac{1}{2}(x-\mu_k)^\intercal \sum\nolimits_{k}^{-1}(x-\mu_k)  + constants \\

\end{aligned}
```
We want to maximize $g(x) = \mathbb{P}(Y = k | X = x) = log(f(x|Y=k)) + log(\pi_{k})$. So $L_{k}(x)= - \frac{1}{2}log|\Sigma_k|^{-1} - \frac{1}{2}(x-\mu_k)^\intercal \sum\nolimits_{k}^{-1}(x-\mu_k) + log(\pi_{k})$ is called [quadratic discriminant function]{style="color:blue"}.

The posterior probabilities of the classes are calculated as follows:

```{=tex}
\begin{aligned}

\mathbb{P}(Y = k | X = x) = \frac{exp(Q_k(x))}{\sum_{\ell=1}^{k}exp(Q_\ell(x))}

\end{aligned}
```
## Limitations

-   LDA and QDA rely on statistical assumptions (normality of data and homogeneity of covariance matrices for LDA). These assumptions can be partially or totally violated in real datasets.

-   Even if LDA or QDA are considered relatively simple models, they can overfit on complex or noisy datasets.

## Practical

```{r}
#| echo: true
#| message: false 

library(caret)
library(MASS)
library(ggplot2)
```

```{r}
#| echo: true
#| message: false 

train <- read.table(file="./data/synth_train.txt", header=TRUE)
head(train)
train$y <- factor(train$y)
train_data <- data.frame(X1 = train$x1, X2 = train$x2, Class = factor(train$y))

ggplot(train_data, 
       aes(x = X1, y = X2, shape = Class, color = Class)) +
geom_point(size = 3) +
scale_color_manual(values = c("red", "blue"), name = "Class") +
scale_shape_manual(values = c(16, 17), name = "Class") +
labs(title = "Visualization of classes", x = "X1", y = "X2") +
theme_minimal() +
theme(legend.position = "top")
```

### Quadratic Discriminant Analysis by Hand

Maximum likelihood estimators

$$
\widehat \pi_k = \frac{n_k}{n}, \qquad 
\widehat \mu_k = \frac{1}{n_k} \sum_{i; y_i=k} x_i,\qquad  \widehat \Sigma_k=\frac{1}{n_k} \sum_{i; y_i=k}(x_i-\widehat \mu_k)^\top (x_i-\widehat \mu_k).
$$ We will estimate these parameters on these parameters on the training data.

```{r}
#| echo: true
#| message: false 
#| code-fold: false

n <- nrow(train_data)
# class 1
ind1 <- which(train_data$Class==1)
n1 <- length(ind1)
pi1 <- n1/n
mu1 <- colMeans(train_data[ind1,1:2])
sigma1 <- var(train_data[ind1,1:2])*(n1-1)/n1

# class 2
ind2 <- which(train_data$Class==2)
n2 <- length(ind2)
pi2 <- n2/n
mu2 <- colMeans(train_data[ind2,1:2])
sigma2 <- var(train_data[ind2,1:2])*(n2-1)/n2
```

We had the **quadratic discriminant function**

$$ 
Q_k(x)=-\frac 12 \log |\Sigma_k|^{-1} - \frac 12 (x-\mu_k)^\top \Sigma_k^{-1}(x-\mu_k)+\log \pi_k.
$$

We have the new data $x = (-1,1)$, then we calculate $Q_1(x)$ and $Q_2(x)$

```{r}
#| echo: true
#| message: false 
#| code-fold: false

x = c(-1,1)
names(x) = c("x1","x2")
Q1 <- - 1/2*log(det(sigma1))-1/2*t(x-mu1) %*% solve(sigma1) %*% (x-mu1) +log(pi1)
Q2 <- - 1/2*log(det(sigma2)) - 1/2*t(x-mu2) %*% solve(sigma2) %*% (x-mu2) +log(pi2)

Q1
Q2
```

We also know that the posterior probabilities of the classes are calculated as follows:

$$
\mathbb{P}(Y=k\vert X=x)= \frac{\exp (Q_k(x))}{\sum_{\ell=1}^2 \exp (Q_\ell(x))}.
$$

Estimate the posterior probabilities for $x = (-1, 1)$.

```{r}
#| echo: true
#| message: false 
#| code-fold: false

prob1 <- exp(Q1)/(exp(Q1)+exp(Q2))
prob1
prob2 <- exp(Q2)/(exp(Q1)+exp(Q2))
prob2
```

$\mathbb{P}(Y=2/X=(-1,1)) > \mathbb{P}(Y=1/X=(-1,1))$ so we predict class 2 for $x=(-1,1)$.

### Quadratic Discriminant Analysis with caret

Now use the functions **qda** and **predict** to predict the class of the point $x=(-1,1)$ and estimate their posterior probabilities. Check that you find the results obtained previously.

Training the model on the train data

```{r}
#| echo: true
#| message: false 
#| code-fold: false

qda_model <- train(Class ~ ., 
                   data = train_data, 
                   method = "qda")
```

Prediction of the point $x = (-1,1)$

```{r}
#| echo: true
#| message: false 
#| warning: false  
#| code-fold: false

X1 = -1
X2 = 1
x = c(X1,X2)
head(predict(qda_model, x, "raw"))
head(predict(qda_model, x, "prob"))
```

Splitting the dataset into training and testing samples

```{r}
#| echo: true
#| message: false 
#| warning: false  
#| code-fold: false

set.seed(1223)

train_index <- createDataPartition(train$y, p = 0.7, list = FALSE)
train_data <- train[train_index, ]
test_data <- train[-train_index, ]
```

Training the model on the training data

```{r}
#| echo: true
#| message: false 
#| warning: false  
#| code-fold: false

qda_model <- train(y ~ ., 
                   data = train_data, 
                   method = "qda")
```

Predictions on the test dataset

```{r}
#| echo: true
#| message: false 
#| warning: false  
#| code-fold: false

qda_pred <- predict(qda_model, test_data)
```

Model Evaluation - Performance Analysis on test dataset

```{r}
#| echo: true
#| message: false 
#| warning: false  
#| code-fold: false

qda_conf_mat <- confusionMatrix(qda_pred, test_data$y)
print(qda_conf_mat)
```

Visualizing the decision boundaries

```{r}
#| echo: true
#| message: false 
#| warning: false  
#| code-fold: false

grid <- expand.grid(x1 = seq(min(test_data$x1), max(test_data$x1), 
                             length.out = 100),
                    x2 = seq(min(test_data$x2), max(test_data$x2), 
                             length.out = 100))

# Decision boundary for QDA
grid$pred <- predict(qda_model, newdata = grid)

ggplot() +
    geom_point(data = test_data, aes(x = x1, y = x2, color = y), size = 3)+
    geom_tile(data = grid, aes(x = x1, y = x2, fill = pred), alpha = 0.3) +
    labs(title = "Decision boundary - QDA", x = "Variable 1", y = "Variable 2") +
    theme_minimal()
```
