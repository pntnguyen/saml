[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics and Machine Learning",
    "section": "",
    "text": "Preface\nThe content of this book is based on the materials of the Spring School on Statistic and Machine Learning, which was held by the Vietnam Institute for Advanced Study in Mathematics (VIASM) and VNU-HCM University of Science (HCMUS).\n\n\n\n\nMaterials\nClassification\nComputational Statistics"
  },
  {
    "objectID": "intro.html#introduction-to-machine-learning",
    "href": "intro.html#introduction-to-machine-learning",
    "title": "Introduction",
    "section": "Introduction to machine learning",
    "text": "Introduction to machine learning\n\nWhat is AI?\nArtificial Intelligent (AI) is a field of computer science and mathematics that brings together a set of algorithmic techniques and theories for creating machines that mimic human intelligence.\nQuestion:\n\nHow do Artificial Intelligent (AI) and machine learning really work?\nHow do they learn from our behaviors, preferences, and interactions ?\n\nThe aim of AI is to simulate human intelligence, and in particular to learn a wide range of tasks. There are two possible ways of learning :\n\nRote learning: explicitly memorizing all possible examples to play them back\nGeneralizing learning: extract implicit rules from a large number of examples to reapply them to new situations never encountered before\n\nRote learning is relatively easy for a machine, as long as the examples are available. On the other hand, learning by generalization is difficult, as it requires the extraction of rules that are not explicitly mentioned in the examples\n\n\n\n\n\n\nRelationship between AI, ML, Neural Networks, and Deep Learning (Sk 2020)\n\n\n\nWhat is machine learning?\nMachine learning is a sub-domain of AI, which involves learning from experience or from a database of implicit rules to answer to a given problem. This field focuses on the statistical analysis of training data. This field focuses on the statistical analysis of training data.\nGenerally speaking, machine learning algorithms are divided into several phases:\n\nTraining phase (or learning phase):\n\nThe chosen model is subjected to a large number of significant examples\nThe system then seeks to learn implicit rules based on this data (called training data)\n\nInference phase:\n\nThe trained model can be used on new inputs\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe training phase generally precedes the use of the model, although some systems continue to learn indefinitely if they have feedback on the results (this is called on-line learning).\nInputs provided during the inference phase can be processed even if they were not seen by the model during the learning phase.\n\n\n\n\n\nType of machine learning\n\nMachine learning uses different types of learning, with supervised learning and unsupervised learning playing a prominent role.\nDeep learning is a set of techniques that use neural networks to solve complex problems.\nReinforcement learning consists in learning by interacting with the agent’s environment."
  },
  {
    "objectID": "intro.html#supervised-and-unsupervised-learning",
    "href": "intro.html#supervised-and-unsupervised-learning",
    "title": "Introduction",
    "section": "Supervised and unsupervised learning",
    "text": "Supervised and unsupervised learning\n\nSupervised learning\nWe have the labeled training data \\((x_{i},y_{i})_{i=1,···,n}\\), the n inputs \\(x_{i}\\) and the associated target outputs \\(y_{i}\\). The aim is to train the chosen model so that it can correctly predict the output for unlabeled inputs.\n\n\n\n\n\nSupervised learning is generally used for regression or classification:\n\nRegression is used when the output to be predicted can take continuous values\nClassification is the task of choosing a class (value) from all those possible.\n\nClassic supervised learning algorithms include linear regression, nearest neighbor algorithms, discriminant factor analysis, logistic regression, neural networks, decision trees, random forests and support vector machines.\n\n\nUnsupervised learning\nWe therefore have input data for which we don’t know the associated output. The data set is therefore \\((x_{i})_{i=1,···,n}\\) and the aim of the system is to identify features common to the training data.\nUnsupervised learning is mainly composed of clustering algorithms. These algorithms seek to separate the input data into a given number of groups. Each element in the group must have characteristics close to those of elements in the same group, but relatively distant from those of other groups.\nThe most common unsupervised learning algorithms are the k-means algorithm, hierarchical ascending classification, principal component analysis, DBSCAN, singular value decomposition and some neural networks.\n\n\n\n\nSk, Singh. 2020. “The Hold-Up of the Century: Neural Networks Are Coming from Cognitive Science and Not Machine Learning. Perspectives to Avoid a New Dark Age of Artificial Intelligence.” Trends in Artificial Intelligence 4 (1). https://doi.org/10.36959/643/306."
  },
  {
    "objectID": "review.html#points-distance",
    "href": "review.html#points-distance",
    "title": "Mathematic Review",
    "section": "2 points distance",
    "text": "2 points distance"
  },
  {
    "objectID": "review.html#covariance-matrix",
    "href": "review.html#covariance-matrix",
    "title": "Mathematic Review",
    "section": "Covariance matrix",
    "text": "Covariance matrix"
  },
  {
    "objectID": "review.html#multivariables-normal-distribution",
    "href": "review.html#multivariables-normal-distribution",
    "title": "Mathematic Review",
    "section": "Multivariables Normal distribution",
    "text": "Multivariables Normal distribution"
  },
  {
    "objectID": "review.html#transpose-matrix",
    "href": "review.html#transpose-matrix",
    "title": "Mathematic Review",
    "section": "Transpose matrix",
    "text": "Transpose matrix"
  },
  {
    "objectID": "review.html#bayesian",
    "href": "review.html#bayesian",
    "title": "Mathematic Review",
    "section": "Bayesian",
    "text": "Bayesian"
  },
  {
    "objectID": "intro_classification.html#introduction-to-classification",
    "href": "intro_classification.html#introduction-to-classification",
    "title": "Classification",
    "section": "Introduction to classification",
    "text": "Introduction to classification\nThe aim of classification is to group (partition, segment) \\(n\\) observations into a number of homogeneous groups or classes. There are two main types of classification :\n\nSupervised classification, often referred to simply as classification\nUnsupervised classification, sometimes called partitioning, segmentation, or clustering."
  },
  {
    "objectID": "intro_classification.html#supervised-classification",
    "href": "intro_classification.html#supervised-classification",
    "title": "Classification",
    "section": "Supervised classification",
    "text": "Supervised classification\nIn supervised classification,\n\nWe already know how many groups exist in the population\nWe know the group to which each observation in the population belongs\nWe want to classify the observations in the right groups based on different variables\n\nWe can then use a classification rule to predict the groups to which new observations belong.\nSome examples of applications:\n\nRecognize hand written numbers\nIdentify the type of cancer patients has\n\nThere are several families of supervised classification methods. The most common are nearest neighbor method, discriminant factor analysis, classification trees, logistic regression, naive Bayesian, neural networks, support vector machines.\nWe have the data with n individuals described by their values of X and Y.\n\\[\\begin{aligned}\nX_n =\n\\begin{bmatrix}\n    x_{11} & x_{12} & x_{13} & \\dots  & x_{1p} \\\\\n    x_{21} & x_{22} & x_{23} & \\dots  & x_{2p} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    x_{n1} & x_{n2} & x_{n3} & \\dots  & x_{np}\n\\end{bmatrix}\n\n\\quad\n,\n\\quad\n\nY_n =\n\\begin{bmatrix}\n    y_{1} \\\\\n    y_{2} \\\\\n    \\vdots \\\\\n    y_{n}\n\\end{bmatrix}\n\\end{aligned}\\]\nWe want to predict the class \\(y_0\\) of a new input \\(x_0 = (x_{01},x_{02},...,x_{0p})\\)"
  },
  {
    "objectID": "intro_classification.html#unsupervised-classification",
    "href": "intro_classification.html#unsupervised-classification",
    "title": "Classification",
    "section": "Unsupervised classification",
    "text": "Unsupervised classification\nIn unsupervised classification,\n\nIn general, we don’t know how many groups exist in the population\nWe don’t know the group to which each observation in the population belongs\nWe want to classify observations into homogeneous groups based on different variables\n\nApplication examples:\n\nIn psychology : the determination of personality types present in a group of individuals\nIn text mining : partitioning e-mails or texts according to subject\n\nThere are several families of unsupervised classification methods. The most common are hierarchical classification, k-means method, density-based classification, mixture of normal distributions."
  },
  {
    "objectID": "pred_per.html#confusion-matrix",
    "href": "pred_per.html#confusion-matrix",
    "title": "1  Predictive performance",
    "section": "1.1 Confusion matrix",
    "text": "1.1 Confusion matrix\nThe confusion matrix counts the occurrences of predictions according to the true values.\n\nwhere \\(n_{kℓ}\\) is the number of observations of class \\(k\\) predicted in the class \\(ℓ\\)"
  },
  {
    "objectID": "pred_per.html#empirical-risk",
    "href": "pred_per.html#empirical-risk",
    "title": "1  Predictive performance",
    "section": "1.2 Empirical risk",
    "text": "1.2 Empirical risk\nThe empirical risk (average cost of misclassification) of the g classification rule is\n\\[\nR(g) = \\frac{1}{n} \\sum_{k=1}^{K}\\sum_{l=1}^{K} C_{kl}n_{kl}\n\\] Where\n\\[\nC_{kl} =\n\\begin{cases}\n1 & k = l \\\\\n0 &  k \\neq l\n\\end{cases}\n\\]\nIn the case of a 0-1 cost, we find the empirical error rate\n\\[\nR(g) = \\frac{1}{n} \\sum_{k=1}^{K}\\sum_{l=1 \\: l\\neq k}^{K} n_{kl}\n\\]"
  },
  {
    "objectID": "pred_per.html#predictive-performances-in-binary-classification",
    "href": "pred_per.html#predictive-performances-in-binary-classification",
    "title": "1  Predictive performance",
    "section": "1.3 Predictive performances in binary classification",
    "text": "1.3 Predictive performances in binary classification\n\n1.3.1 Precision/Recall/Specificity\n\n\nConfusion matrix in binary classification\n\n\nTrue positive rate (TPR) is also called sensitivity, recall.\nThe false positive rate (FPR) corresponds to 1 - specificity.\nThe postive predictive value (PPV) is also called precision.\n\n\n\n1.3.2 F1- Score\nIn binary classification, the F1-score depends on:\n\nThe positive predictive value (PPV), also called precision.\nThe true positive rate (TPR), also called sensitivity, recall.\n\n\\[\nF_{1} = \\frac{2PPV \\times TPR}{PPV + TPR} = \\frac{2TP}{2TP + FP + FN}\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\\begin{align}\nF_{1} &= \\frac{2PPV \\times TPR}{PPV + TPR} \\\\\n      &= \\frac{2 \\frac{TP}{TP+FP}\\times\\frac{TP}{TP+FN}}{\\frac{TP}{TP+FP} + \\frac{TP}{TP+FN}} \\\\\n\nNumerator &=  2 \\frac{TP^2}{(TP+FP)(TP+Fn)} \\\\\n\nDenominator &= \\frac{TP(TP+FN)+TP(TP+FP)}{(TP+FP)(TP+FN)} = \\frac{TP(2TP+FN+FP)}{(TP+FP)(TP+FN)} \\\\\n\nF_{1} &= \\frac{2TP^{2}}{(TP+FP)(TP+FN)} \\cdot  \\frac{(TP+FP)(TP+FN)}{TP(2TP+FN+FP)}\\\\\n\n      &= \\frac{2TP^{2}}{TP(2TP+FN+FP)}\\\\\n      \n      &= \\frac{2TP}{2TP+FN+FP}\n\n\\end{align}\\]\n\n\n\nIt measures the classification rule’s ability to correctly predict class 1 entries and not predict 1 of the class 2 entries.\nIn the case where the predictions are no longer binary (multi-class), the F-measure is calculated by making the average of F1 scores for each class.\n\n\n1.3.3 Kappa de Cohen\nIn statistics, the kappa method (kappa) measures agreement between observers during qualitative coding into categories.\n\\[\n\\kappa = \\frac{Pr(a) - Pr(e)}{1 - Pr(e)}\n\\]\nWhere:\n\n\\(Pr(a)\\) is the proportion of agreement between coders\n\\(Pr(e)\\) is the proportion of a random agreement\n\n\n\n\n\n\n\nMarc and Simon are responsible for defining who will be accepted or not at the final exam in a group of 50 students. Each of them checks the copy of each student and notes received or not (YES or NO)\n\n\\[\nP_{a} = \\frac {a+d}{a+b+c+d} = \\frac{20+15}{50}\n\\]\nTo calculate the probability of agreement “at random”, we note :\n\nSimon scored YES to 25 students, or 50% of the cases.\nMarc scored YES in 60%, 30 out of 50 students.\n\n\\[\nP_{YES} = \\frac{a+b}{a+b+c+d} \\times \\frac{a+c}{a+b+c+d} = 0.5 \\times 0.6 = 0.3\n\\]\n\\[\nP_{NO} = \\frac{c+d}{a+b+c+d} \\times \\frac{b+d}{a+b+c+d} = 0.5 \\times 0.4 = 0.2\n\\]\nThe global probability that the teachers agree is:\n\\[\nP_{e} = P_{YES} + P_{NO} = 0.3 + 0.2 = 0.5\n\\]\nKappa’s formula then gives :\n\\[\n\\kappa = \\frac{Pr(a) - Pr(e)}{1 - Pr(e)} = \\frac{0.7 - 0.5}{1 - 0.5} = 0.4\n\\]\n\n\n1.3.4 ROC curve\n\nThe ROC curve is a graphical representation used to evaluate the performance of a binary classification model. It illustrates the trade-off between sensitivity (TPR) and specificity (1 - FPR) on different threshold parameters.\nThe shape of the ROC curve gives an overview of the efficiency of a classification model. A curve that slopes towards the upper left corner indicates a model with high sensitivity and specificity, while a curve closer to the diagonal line suggests a model with low discriminative power.\nThe area under the ROC curve (AUC) is an essential measure of model performance. It is calculated by integrating the area under the ROC curve, providing a single scalar value that summarizes the model’s ability to distinguish between classes. An AUC:\n\nof 0.5 suggests absence of discriminating ability\nfrom 0.7 to 0.8 is considered as acceptable\ngreater than 0.8 indicates good performance\ngreater than 0.9 suggests excellent performance\nof 1 indicates perfect classification\n\n\n\n\n\n\n\n\nTake home message\n\n\n\nA good model is both:\n\nsensitive and specificity. This is measured with the ROC curve and the AUC.\nsensible and accurate. This is measured with the F1-measure."
  },
  {
    "objectID": "validation.html#cross-validation",
    "href": "validation.html#cross-validation",
    "title": "2  Learning, validation, testing",
    "section": "2.1 Cross-validation",
    "text": "2.1 Cross-validation\nCross-validation is a method of estimating model reliability based on a sampling technique.\n\n2.1.1 K-fold cross-validation\nThe original sample is divided into K samples (or blocks), then one of the K samples is selected as the validation set, while the other K − 1 samples constitute the training set.\n\nSplit the data into K sub-samples of the same size\nFor \\(k = 1,··· ,K,\\):\n\nestimate the rule on the private data of sample k\npredict the data of sample k with this rule\n\nCompute the performance criterion on these K predictions\n\n\n\n2.1.2 Leave-one-out cross-validation (LOOCV)\nLeave-one-out cross-validation is the special case of K block cross-validation with K = n. That is, at each learning-validation iteration, learning is performed on \\(n − 1\\) observations and validation on the single remaining observation.\n\nFor \\(i = 1,··· ,n\\)\n\nestimate the rule on the data without the \\(i_{th}\\) data\npredict this data i with this rule\n\nCompute the performance criterion on these n predictions"
  },
  {
    "objectID": "knn.html#principle",
    "href": "knn.html#principle",
    "title": "3  k-nearest neighbors (kNN)",
    "section": "3.1 Principle",
    "text": "3.1 Principle\n\nFor each new input \\(x_0\\), we measure the distance between \\(x_0\\) and the inputs \\(x_i\\) for all \\(i\\) in \\({1,··· ,n}\\).\n\n\n\n\n\n\n\nNote\n\n\n\nDifferent distance measures (Euclidean, Manhattan…) can be used for this algorithm, depending on the problem studied.\n\\[\\begin{align}\n\nA(x_A,y_A,z_A)\\:\n\nB(x_B,y_B,z_B)\\\\\n\nd_{Euclidean}(A,B) &=  \\sqrt{(x_B - x_A)^2 + (y_B - y_A)^2 + (z_B - z_A)^2} \\\\\n\nd_{Manhattan}(A,B) &=  |x_{B} - x_A| + |y_B - y_A| + |z_B - z_A|\n\n\\end{align}\\]\n\n\n\nWe then select the \\(k\\) elements closest to the input.\nThe class predicted by the algorithm then corresponds to the majority class, i.e. the most frequent class in the k-NNs selected."
  },
  {
    "objectID": "knn.html#applications",
    "href": "knn.html#applications",
    "title": "3  k-nearest neighbors (kNN)",
    "section": "3.2 Applications",
    "text": "3.2 Applications\n\nThe k-NN algorithm is frequently used in both classification and regression.\nIt can also be used in shape recognition, with inputs containing the circumference, area or contour signature of the shape, and outputs corresponding to the various possible shapes."
  },
  {
    "objectID": "knn.html#limitations",
    "href": "knn.html#limitations",
    "title": "3  k-nearest neighbors (kNN)",
    "section": "3.3 Limitations",
    "text": "3.3 Limitations\n\n\n\n\n\n\nWarning\n\n\n\nThe value of k has a strong influence on prediction !\n\n\n\nk too small: elements out of the ordinary will more easily influence the prediction. =&gt; This is the overfitting problem.\nk too large: the model will take into account data that are too far apart, and the majority class will be predicted too often. =&gt; This is the underfitting problem.\n\nWhen studying higher-dimensional problems, it is therefore essential to have a large amount of training data. For this reason, the k-NN algorithm quickly becomes unusable : beyond 4 or 5 dimensions, the number of data required becomes too large."
  },
  {
    "objectID": "knn.html#practical",
    "href": "knn.html#practical",
    "title": "3  k-nearest neighbors (kNN)",
    "section": "3.4 Practical",
    "text": "3.4 Practical\n\n3.4.1 Example 1\nWe have a data of 10 points\n\n\n\n\n\nnumber\nx\ny\nz\ncolor\n\n\n\n\n1\n3\n7\n5\nblack\n\n\n2\n4\n6\n2\nblack\n\n\n3\n3\n7\n8\nwhite\n\n\n4\n0\n1\n2\nblack\n\n\n5\n1\n0\n7\nwhite\n\n\n6\n5\n4\n4\nwhite\n\n\n7\n9\n1\n2\nblack\n\n\n8\n5\n3\n3\nblack\n\n\n9\n1\n1\n4\nwhite\n\n\n10\n2\n3\n7\nwhite\n\n\n\n\nWe consider an eleventh point A with coordinates x = 4, y = 4 and z = 5\nThe Euclidean distances between the point A(4,4,5) and the points in the table have been recorded in the following table:\n\n\n\npoint\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\ndistance\n3.16\n3.60\n4.16\n5.83\n5.19\n1.41\n6.55\n2.45\n4.16\n3\n\n\n\n\n\nWe apply the k-nearest neighbors algorithm to predict the color at point A.\n\nIf k = 1:\n\nThe closet point from A is point number 6 =&gt; A is white\n\nIf k = 3:\n\nThe 3 closet points from A are number 6 (white), number 8 (black), number 10 (black) =&gt; A is black\n\nIf k = 5:\n\nThe 5 closet points from A are number 6 (white), number 8 (black), number 10 (black), number 1 (black), number 2 (black) =&gt; A is black\n\n\n3.4.2 Example 2\n\nlibrary(caret)\nlibrary(MASS)\nlibrary(ggplot2)\n\n\nData\n\ntrain &lt;- read.table(file=\"./data/synth_train.txt\", header=TRUE)\n\nhead(train)\n\n  y          x1        x2\n1 2 -0.72221141 2.0044709\n2 2 -0.92467912 0.4836693\n3 2 -0.76602281 0.7943289\n4 2 -0.07328948 0.9699291\n5 1 -1.39291198 0.9996971\n6 2 -0.20223339 1.3503319\n\nXtrain &lt;- train[,-1]\nYtrain &lt;- train$y\n\ntrain$y &lt;- factor(train$y)\ntrain_data &lt;- data.frame(X1 = train$x1, X2 = train$x2, Class = factor(train$y))\n\nggplot(train_data, \n       aes(x = X1, y = X2, shape = Class, color = Class)) +\ngeom_point(size = 3) +\nscale_color_manual(values = c(\"#8785B2FF\", \"#DABD61FF\"), name = \"Class\") +\nscale_shape_manual(values = c(16, 17), name = \"Class\") +\nlabs(title = \"Visualization of classes\", x = \"X1\", y = \"X2\") +\ntheme_minimal() +\ntheme(legend.position = \"top\")\n\n\n\n\n\n\nTrain model\n\nmodel_15 &lt;- train(\n    y ~ .,\n    data = train, # Data\n    method = \"knn\", # k-nn algorithm\n    tuneGrid = data.frame(k = 15) # Number of neighbors\n)\n\n\na &lt;- seq(from=min(train$x1), to=max(train$x1), length.out=100)\nb &lt;- seq(from=min(train$x2), to=max(train$x2), length.out=100)\ngrille &lt;- NULL\nfor (i in a){\ngrille &lt;- data.frame(rbind(grille, cbind(i,b)))\n}\n\nnames(grille) = c(\"x1\",\"x2\")\n\ntrain_data &lt;- data.frame(Xtrain, Ytrain = factor(Ytrain))\npred_grille &lt;- predict(model_15, grille, type = \"raw\")\n\ngrid_data &lt;- data.frame(grille, pred_grille = factor(pred_grille))\n\n\nggplot() +\n  geom_point(data = grid_data,\n             aes(x = grille[,1], y = grille[,2], color = pred_grille),\n             alpha = 0.5, size = 0.5)+\n  geom_point(data = train_data,\n             aes(x = x1, y = x2, shape = Ytrain, color = Ytrain),size = 3) +\n  scale_color_manual(values = c(\"#8785B2FF\", \"#DABD61FF\"), name = \"Class\") +\n  scale_shape_manual(values = c(16, 17), name = \"Class\") +\n  labs(title = \"Decision frontier for k=15 neighbors\", x = \"X1\", y = \"X2\") +\n  theme_minimal() +\n  theme(legend.position = \"topright\")\n\n\n\n\n\n\nPerformance test\n\npredictions = predict(model_15, Xtrain ,type = \"raw\")\nCM_15 = confusionMatrix(predictions, train$y)\nCM_15\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  1  2\n         1 15  1\n         2  7 77\n                                          \n               Accuracy : 0.92            \n                 95% CI : (0.8484, 0.9648)\n    No Information Rate : 0.78            \n    P-Value [Acc &gt; NIR] : 0.0001699       \n                                          \n                  Kappa : 0.7416          \n                                          \n Mcnemar's Test P-Value : 0.0770999       \n                                          \n            Sensitivity : 0.6818          \n            Specificity : 0.9872          \n         Pos Pred Value : 0.9375          \n         Neg Pred Value : 0.9167          \n             Prevalence : 0.2200          \n         Detection Rate : 0.1500          \n   Detection Prevalence : 0.1600          \n      Balanced Accuracy : 0.8345          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\n\n\nValidation\n\nvalid &lt;- read.table(file=\"./data/synth_valid.txt\", header=TRUE)\nvalid$y = as.factor(valid$y)\nhead(valid)\n\n  y          x1        x2\n1 2  0.54837733 1.2213453\n2 2 -0.51618236 1.5623959\n3 2 -0.92877833 0.9210722\n4 2  0.07000405 0.6197675\n5 2  0.26702843 1.1094406\n6 2 -0.57664073 1.0257432\n\n\n\npredictions = predict(model_15, valid ,type = \"raw\")\nCM_15 = confusionMatrix(predictions, valid$y)\nCM_15\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  1  2\n         1 27  0\n         2 10 63\n                                         \n               Accuracy : 0.9            \n                 95% CI : (0.8238, 0.951)\n    No Information Rate : 0.63           \n    P-Value [Acc &gt; NIR] : 8.883e-10      \n                                         \n                  Kappa : 0.7728         \n                                         \n Mcnemar's Test P-Value : 0.004427       \n                                         \n            Sensitivity : 0.7297         \n            Specificity : 1.0000         \n         Pos Pred Value : 1.0000         \n         Neg Pred Value : 0.8630         \n             Prevalence : 0.3700         \n         Detection Rate : 0.2700         \n   Detection Prevalence : 0.2700         \n      Balanced Accuracy : 0.8649         \n                                         \n       'Positive' Class : 1              \n                                         \n\n\n\n\nPreprocessing data\nNot preprocessing data can also lead to an overestimation of performance measures in k-nns. Let’s illustrate this phenomenon on these data\n\nset.seed(123) # For reproducibility\ngrid &lt;- expand.grid(k = seq(1, 30, by = 1)) \n\nknn_fit_raw &lt;- train(y ~ ., # Formula: target variable and predictors\n                     data = train, # Dataset\n                     method = \"knn\", # k-nn algorithm\n                     tuneGrid = grid) # Grid of values for k\n\n# Training the k-nn model with pre-processing\nknn_fit_PP &lt;- train(\n  y ~ ., # Formula: target variable and predictors\n  data = train, # Dataset\n  method = \"knn\", # k-nn algorithm\n  preProcess = c(\"center\", \"scale\"),\n  tuneGrid = grid) # Grid of values for k\n\nk_values_raw &lt;- knn_fit_raw$results$k\naccuracy_raw &lt;- knn_fit_raw$results$Accuracy\n\nk_values_PP &lt;- knn_fit_PP$results$k\naccuracy_PP &lt;- knn_fit_PP$results$Accuracy\n\n# Construction of a dataframe collecting the data\n\ndata_raw &lt;- data.frame(k_values = k_values_raw,\n                       accuracy = accuracy_raw,\n                       Data = \"Raw Data\")\n\ndata_pp &lt;- data.frame(k_values = k_values_PP,\n                      accuracy = accuracy_PP,\n                      Data = \"Preprocessed Data\")\n\nplot_data &lt;- rbind(data_raw, data_pp)\n\n\n\nCode\nggplot(plot_data, \n       aes(x = k_values,y = accuracy,\n           color = Data,linetype = Data,shape = Data)) +\n  geom_line(lwd = 1) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Precision depending on the number of neighbors (k)\",\n    x = \"Number of neighbors (k)\",\n    y = \"Precision\",\n    color = \"Data Type\",\n    linetype = \"Data Type\",\n    shape = \"Data Type\") +\n  scale_color_manual(values = c(\"blue\", \"red\")) +\n  scale_linetype_manual(values = c(1, 2)) +\n  scale_shape_manual(values = c(16, 16)) +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nOverfitting\n\nmodel_1 &lt;- train(\n  y ~ ., # Formula: target variable and predictors\n  data = train, # Dataset\n  method = \"knn\", # k-nn algorithm\n  tuneGrid = data.frame(k = 1) # Number of neighbors\n)\n\n# Plot of the decision boundary for k = 1 neighbor\n\ntrain_data &lt;- data.frame(Xtrain, Ytrain = factor(Ytrain))\npred_grille &lt;- predict(model_1, grille, type = \"raw\")\ngrid_data &lt;- data.frame(grille, pred_grille = factor(pred_grille))\n\nggplot() +\n  geom_point(data = grid_data,\n             aes(x = grille[,1], y = grille[,2], color = pred_grille),\n             alpha = 0.5, size = 0.5) +\n  geom_point(data = train_data,\n  aes(x = Xtrain[,1], y = Xtrain[,2],\n      shape = Ytrain, color = Ytrain), size = 3) +\n  scale_color_manual(values = c(\"#8785B2FF\", \"#DABD61FF\"), name = \"Class\") +\n  scale_shape_manual(values = c(16, 17), name = \"Class\") +\n  labs(title = \"Decision boundary for k=1 neighbor\", x = \"X1\", y = \"X2\") +\n  theme_minimal() +\n  theme(legend.position = \"topright\")\n\n\n\n\n\n\nFinding the optimal number of classes by cross-validation\n\nset.seed(123)\ngrid &lt;- expand.grid(k = seq(1, 30, by = 1)) # Grid of k values\n\nknn_model_validation &lt;- train(\n  y ~ ., data = train, method = \"knn\",\n  trControl = trainControl(method = \"cv\", number = 10) ,\n  tuneGrid = grid # Test k = 1, 2, ..., 30\n  )\n\nknn_model_validation\n\nk-Nearest Neighbors \n\n100 samples\n  2 predictor\n  2 classes: '1', '2' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 89, 90, 90, 90, 90, 90, ... \nResampling results across tuning parameters:\n\n  k   Accuracy   Kappa    \n   1  0.9318182  0.7884278\n   2  0.9095960  0.7048375\n   3  0.9416162  0.8126161\n   4  0.9427273  0.8254308\n   5  0.9205051  0.7350241\n   6  0.9407071  0.8064679\n   7  0.9295960  0.7673375\n   8  0.9327273  0.7748235\n   9  0.9105051  0.6965626\n  10  0.9114141  0.7027108\n  11  0.9205051  0.7350241\n  12  0.9205051  0.7350241\n  13  0.9105051  0.6965626\n  14  0.9114141  0.7027108\n  15  0.9105051  0.6965626\n  16  0.9014141  0.6642493\n  17  0.9014141  0.6642493\n  18  0.9114141  0.6905650\n  19  0.9105051  0.6844168\n  20  0.9105051  0.6844168\n  21  0.9105051  0.6844168\n  22  0.9014141  0.6521035\n  23  0.9105051  0.6844168\n  24  0.9105051  0.6844168\n  25  0.9014141  0.6521035\n  26  0.9105051  0.6844168\n  27  0.9014141  0.6521035\n  28  0.9014141  0.6521035\n  29  0.9014141  0.6521035\n  30  0.8714141  0.4674881\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was k = 4.\n\n\n\n\nTraining of the final model on the training data + the validation data\n\nbest_k &lt;- knn_model_validation$bestTune$k\n\nfinalData &lt;- rbind(train, valid) # Union of the datasets\n\nset.seed(123)\n\nfinal_model &lt;- train(\n  y ~ ., data = finalData,\n  method = \"knn\",\n  tuneGrid = data.frame(k = best_k), # Use the best k\n  trControl = trainControl(method = \"none\")\n)\n\nImport test data\n\ntest &lt;- read.table(file=\"./data/synth_test.txt\", header=TRUE)\ntest$y = as.factor(test$y)\nhead(test)\n\n  y          x1        x2\n1 2 -0.30748160 1.1420242\n2 2  0.51837239 1.0574169\n3 2 -0.05222164 0.7100728\n4 2 -0.08914077 1.3942948\n5 2 -0.37621346 0.9307517\n6 2 -0.18555218 0.7054576\n\ntestPredictions &lt;- predict(final_model, newdata = test)\nconfusionMatrix(testPredictions, test$y)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  1  2\n         1 24  1\n         2  1 74\n                                          \n               Accuracy : 0.98            \n                 95% CI : (0.9296, 0.9976)\n    No Information Rate : 0.75            \n    P-Value [Acc &gt; NIR] : 1.874e-10       \n                                          \n                  Kappa : 0.9467          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.9600          \n            Specificity : 0.9867          \n         Pos Pred Value : 0.9600          \n         Neg Pred Value : 0.9867          \n             Prevalence : 0.2500          \n         Detection Rate : 0.2400          \n   Detection Prevalence : 0.2500          \n      Balanced Accuracy : 0.9733          \n                                          \n       'Positive' Class : 1"
  },
  {
    "objectID": "lda_qda.html#principle",
    "href": "lda_qda.html#principle",
    "title": "4  Discriminant analysis",
    "section": "4.1 Principle",
    "text": "4.1 Principle\nWe have a sample of \\(n\\) observations distributed in \\(K\\) groups of sizes \\(n_k\\). The number of classes \\(K\\) is fixed in advance.\n\\[\\begin{aligned}\nX_n =\n\\begin{bmatrix}\n    x_{11} & x_{12} & x_{13} & \\dots  & x_{1p} \\\\\n    x_{21} & x_{22} & x_{23} & \\dots  & x_{2p} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    x_{n1} & x_{n2} & x_{n3} & \\dots  & x_{np}\n\\end{bmatrix}\n\n\\quad\n,\n\\quad\n\nY_n =\n\\begin{bmatrix}\n    y_{1} \\\\\n    y_{2} \\\\\n    \\vdots \\\\\n    y_{n}\n\\end{bmatrix}\\\\\n\ny \\in \\{1,...,K\\}\n\n\\end{aligned}\\]\nIn discriminant analysis, we assume that the conditional distribution of X to class Y is parametric and Gaussian \\((X|Y = k \\sim \\mathcal{N}(\\mu_{K},\\Sigma_{K}))\\)\nWhere:\n\n\\(\\mu_{k}\\): the centers of gravity of the conditional point clouds\n\\(\\Sigma_k\\): their variance-covariance matrix\n\n\n\n\n\n\n\nNote\n\n\n\nThen the probability density function (PDF) is:\n\\[\\begin{aligned}\n\nf(x|Y=k) = \\frac{1}{(2 \\pi)^\\frac{p}{2}|\\Sigma_{k}|^{\\frac{1}{2}}}exp\\{ -\\frac{1}{2}(x-\\mu_k)^\\intercal \\sum\\nolimits_{k}^{-1}(x-\\mu_k)\\}\n\n\\end{aligned}\\]\n\n\n\n\n\n\n\n\nLinear discriminant analysis\n\n\n\n\n\n\n\nQuadratic discriminant analysis\n\n\n\n\n\nIn training step, we want to find the classification rule \\(g(x)\\) that maximize the \\(\\mathbb{P}(Y = k | X = x)\\). The Bayesian rule write as:\n\\[\\begin{aligned}\n\ng(x) &= \\mathbb{P}(Y = k | X = x) \\\\\n     \n     &= f(x|Y=k) \\mathbb{P}(Y=k)\\\\\n     \n     &= log(f(x|Y=k)) + log(\\pi_{k})\\\\\n\n\\end{aligned}\\]\nMaximum likelihood estimators\n\\[\\begin{aligned}\n\n\\widehat{\\pi} &= \\frac{n_k}{n} &  \\widehat{\\mu_k} = \\frac{1}{n_k}\\sum_{i;y_i = k }^{}x_i\\\\\n     \n    \\widehat{\\Sigma_k} &= \\frac{1}{n_k}\\sum_{i;y_i = k }^{}(x_i-\\widehat{\\mu_k})^\\intercal (x_i-\\widehat{\\mu_k})\\\\\n\n\\end{aligned}\\]"
  },
  {
    "objectID": "lda_qda.html#linear-discriminant-analysis-lda",
    "href": "lda_qda.html#linear-discriminant-analysis-lda",
    "title": "4  Discriminant analysis",
    "section": "4.2 Linear Discriminant analysis (LDA)",
    "text": "4.2 Linear Discriminant analysis (LDA)\nWe assume that K clusters have the same \\(\\Sigma\\), \\(\\Sigma_k = \\Sigma\\) for k = 1,··· ,K. Now the likelihood is\n\\[\\begin{aligned}\n\nf(x|Y=k) = \\frac{1}{(2 \\pi)^\\frac{p}{2}|\\Sigma|^{\\frac{1}{2}}}exp\\{ -\\frac{1}{2}(x-\\mu_k)^\\intercal \\sum\\nolimits_{}^{-1}(x-\\mu_k)\\}\n\n\\end{aligned}\\]\ntake the log\n\\[\\begin{aligned}\n\nlog(f(x|Y=k)) &= \\underbrace{- \\frac{\\pi}{2} log(2 \\pi) - \\frac{1}{2}log|\\Sigma|} - \\frac{1}{2}(x-\\mu_k)^\\intercal \\sum\\nolimits_{}^{-1}(x-\\mu_k) \\\\\n\n&= independent\\ on\\ k \\\\\n\n&= - \\frac{1}{2}(x-\\mu_k)^\\intercal \\sum\\nolimits_{}^{-1}(x-\\mu_k)  + constants \\\\\n\n&= - \\frac{1}{2}(x^\\intercal \\Sigma^{-1}x -2x^\\intercal \\Sigma^{-1}\\mu_k+\\mu^{\\intercal}_k \\sum\\nolimits_{}^{-1}\\mu_k) + constants \\\\\n\n&=  \\underbrace{-\\frac{1}{2}x^\\intercal \\Sigma^{-1}x} + x^\\intercal \\Sigma^{-1}\\mu_k -\\frac{1}{2} \\mu^{\\intercal}_k \\sum\\nolimits_{}^{-1}\\mu_k + constants \\\\\n\n&= same\\ for\\ all\\ k\n\n\\end{aligned}\\]\nWe want to maximize \\(g(x) = \\mathbb{P}(Y = k | X = x) = log(f(x|Y=k)) + log(\\pi_{k})\\). So \\(L_{k}(x)= x^\\intercal \\Sigma^{-1}\\mu_k -\\frac{1}{2} \\mu^{\\intercal}_k \\sum\\nolimits_{}^{-1}\\mu_k + log(\\pi_{k})\\) is called linear discriminant function.\nThe posterior probabilities of the classes are calculated as follows:\n\\[\\begin{aligned}\n\n\\mathbb{P}(Y = k | X = x) = \\frac{exp(L_k(x))}{\\sum_{\\ell=1}^{k}exp(L_\\ell(x))}\n\n\\end{aligned}\\]"
  },
  {
    "objectID": "lda_qda.html#quadratic-discriminant-analysis-qda",
    "href": "lda_qda.html#quadratic-discriminant-analysis-qda",
    "title": "4  Discriminant analysis",
    "section": "4.3 Quadratic Discriminant analysis (QDA)",
    "text": "4.3 Quadratic Discriminant analysis (QDA)\nWe no longer assume that K clusters have the same \\(\\Sigma\\), \\(\\Sigma_k = \\Sigma\\) for k = 1,··· ,K. Take the log likelihood of PDF of multivariate Gaussian distribution\n\\[\\begin{aligned}\n\nlog(f(x|Y=k)) &= \\underbrace{- \\frac{\\pi}{2} log(2 \\pi)} - \\frac{1}{2}log|\\Sigma_k|^{-1} - \\frac{1}{2}(x-\\mu_k)^\\intercal \\sum\\nolimits_{k}^{-1}(x-\\mu_k) \\\\\n\n&= independent\\ on\\ k \\\\\n\n&= - \\frac{1}{2}log|\\Sigma_k|^{-1} - \\frac{1}{2}(x-\\mu_k)^\\intercal \\sum\\nolimits_{k}^{-1}(x-\\mu_k)  + constants \\\\\n\n\\end{aligned}\\]\nWe want to maximize \\(g(x) = \\mathbb{P}(Y = k | X = x) = log(f(x|Y=k)) + log(\\pi_{k})\\). So \\(L_{k}(x)= - \\frac{1}{2}log|\\Sigma_k|^{-1} - \\frac{1}{2}(x-\\mu_k)^\\intercal \\sum\\nolimits_{k}^{-1}(x-\\mu_k) + log(\\pi_{k})\\) is called quadratic discriminant function.\nThe posterior probabilities of the classes are calculated as follows:\n\\[\\begin{aligned}\n\n\\mathbb{P}(Y = k | X = x) = \\frac{exp(Q_k(x))}{\\sum_{\\ell=1}^{k}exp(Q_\\ell(x))}\n\n\\end{aligned}\\]"
  },
  {
    "objectID": "lda_qda.html#limitations",
    "href": "lda_qda.html#limitations",
    "title": "4  Discriminant analysis",
    "section": "4.4 Limitations",
    "text": "4.4 Limitations\n\nLDA and QDA rely on statistical assumptions (normality of data and homogeneity of covariance matrices for LDA). These assumptions can be partially or totally violated in real datasets.\nEven if LDA or QDA are considered relatively simple models, they can overfit on complex or noisy datasets."
  },
  {
    "objectID": "lda_qda.html#practical",
    "href": "lda_qda.html#practical",
    "title": "4  Discriminant analysis",
    "section": "4.5 Practical",
    "text": "4.5 Practical\n\n\nCode\nlibrary(caret)\nlibrary(MASS)\nlibrary(ggplot2)\n\n\n\n\nCode\ntrain &lt;- read.table(file=\"./data/synth_train.txt\", header=TRUE)\nhead(train)\n\n\n  y          x1        x2\n1 2 -0.72221141 2.0044709\n2 2 -0.92467912 0.4836693\n3 2 -0.76602281 0.7943289\n4 2 -0.07328948 0.9699291\n5 1 -1.39291198 0.9996971\n6 2 -0.20223339 1.3503319\n\n\nCode\ntrain$y &lt;- factor(train$y)\ntrain_data &lt;- data.frame(X1 = train$x1, X2 = train$x2, Class = factor(train$y))\n\nggplot(train_data, \n       aes(x = X1, y = X2, shape = Class, color = Class)) +\ngeom_point(size = 3) +\nscale_color_manual(values = c(\"red\", \"blue\"), name = \"Class\") +\nscale_shape_manual(values = c(16, 17), name = \"Class\") +\nlabs(title = \"Visualization of classes\", x = \"X1\", y = \"X2\") +\ntheme_minimal() +\ntheme(legend.position = \"top\")\n\n\n\n\n\n\n4.5.1 Quadratic Discriminant Analysis by Hand\nMaximum likelihood estimators\n\\[\n\\widehat \\pi_k = \\frac{n_k}{n}, \\qquad\n\\widehat \\mu_k = \\frac{1}{n_k} \\sum_{i; y_i=k} x_i,\\qquad  \\widehat \\Sigma_k=\\frac{1}{n_k} \\sum_{i; y_i=k}(x_i-\\widehat \\mu_k)^\\top (x_i-\\widehat \\mu_k).\n\\] We will estimate these parameters on these parameters on the training data.\n\nn &lt;- nrow(train_data)\n# class 1\nind1 &lt;- which(train_data$Class==1)\nn1 &lt;- length(ind1)\npi1 &lt;- n1/n\nmu1 &lt;- colMeans(train_data[ind1,1:2])\nsigma1 &lt;- var(train_data[ind1,1:2])*(n1-1)/n1\n\n# class 2\nind2 &lt;- which(train_data$Class==2)\nn2 &lt;- length(ind2)\npi2 &lt;- n2/n\nmu2 &lt;- colMeans(train_data[ind2,1:2])\nsigma2 &lt;- var(train_data[ind2,1:2])*(n2-1)/n2\n\nWe had the quadratic discriminant function\n\\[\nQ_k(x)=-\\frac 12 \\log |\\Sigma_k|^{-1} - \\frac 12 (x-\\mu_k)^\\top \\Sigma_k^{-1}(x-\\mu_k)+\\log \\pi_k.\n\\]\nWe have the new data \\(x = (-1,1)\\), then we calculate \\(Q_1(x)\\) and \\(Q_2(x)\\)\n\nx = c(-1,1)\nnames(x) = c(\"x1\",\"x2\")\nQ1 &lt;- - 1/2*log(det(sigma1))-1/2*t(x-mu1) %*% solve(sigma1) %*% (x-mu1) +log(pi1)\nQ2 &lt;- - 1/2*log(det(sigma2)) - 1/2*t(x-mu2) %*% solve(sigma2) %*% (x-mu2) +log(pi2)\n\nQ1\n\n          [,1]\n[1,] -1.940979\n\nQ2\n\n           [,1]\n[1,] -0.9362387\n\n\nWe also know that the posterior probabilities of the classes are calculated as follows:\n\\[\n\\mathbb{P}(Y=k\\vert X=x)= \\frac{\\exp (Q_k(x))}{\\sum_{\\ell=1}^2 \\exp (Q_\\ell(x))}.\n\\]\nEstimate the posterior probabilities for \\(x = (-1, 1)\\).\n\nprob1 &lt;- exp(Q1)/(exp(Q1)+exp(Q2))\nprob1\n\n          [,1]\n[1,] 0.2680105\n\nprob2 &lt;- exp(Q2)/(exp(Q1)+exp(Q2))\nprob2\n\n          [,1]\n[1,] 0.7319895\n\n\n\\(\\mathbb{P}(Y=2/X=(-1,1)) &gt; \\mathbb{P}(Y=1/X=(-1,1))\\) so we predict class 2 for \\(x=(-1,1)\\).\n\n\n4.5.2 Quadratic Discriminant Analysis with caret\nNow use the functions qda and predict to predict the class of the point \\(x=(-1,1)\\) and estimate their posterior probabilities. Check that you find the results obtained previously.\nTraining the model on the train data\n\nqda_model &lt;- train(Class ~ ., \n                   data = train_data, \n                   method = \"qda\")\n\nPrediction of the point \\(x = (-1,1)\\)\n\nX1 = -1\nX2 = 1\nx = c(X1,X2)\nhead(predict(qda_model, x, \"raw\"))\n\n[1] 2\nLevels: 1 2\n\nhead(predict(qda_model, x, \"prob\"))\n\n          1         2\n1 0.2679847 0.7320153\n\n\nSplitting the dataset into training and testing samples\n\nset.seed(1223)\n\ntrain_index &lt;- createDataPartition(train$y, p = 0.7, list = FALSE)\ntrain_data &lt;- train[train_index, ]\ntest_data &lt;- train[-train_index, ]\n\nTraining the model on the training data\n\nqda_model &lt;- train(y ~ ., \n                   data = train_data, \n                   method = \"qda\")\n\nPredictions on the test dataset\n\nqda_pred &lt;- predict(qda_model, test_data)\n\nModel Evaluation - Performance Analysis on test dataset\n\nqda_conf_mat &lt;- confusionMatrix(qda_pred, test_data$y)\nprint(qda_conf_mat)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  1  2\n         1  6  1\n         2  0 22\n                                          \n               Accuracy : 0.9655          \n                 95% CI : (0.8224, 0.9991)\n    No Information Rate : 0.7931          \n    P-Value [Acc &gt; NIR] : 0.01031         \n                                          \n                  Kappa : 0.901           \n                                          \n Mcnemar's Test P-Value : 1.00000         \n                                          \n            Sensitivity : 1.0000          \n            Specificity : 0.9565          \n         Pos Pred Value : 0.8571          \n         Neg Pred Value : 1.0000          \n             Prevalence : 0.2069          \n         Detection Rate : 0.2069          \n   Detection Prevalence : 0.2414          \n      Balanced Accuracy : 0.9783          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nVisualizing the decision boundaries\n\ngrid &lt;- expand.grid(x1 = seq(min(test_data$x1), max(test_data$x1), \n                             length.out = 100),\n                    x2 = seq(min(test_data$x2), max(test_data$x2), \n                             length.out = 100))\n\n# Decision boundary for QDA\ngrid$pred &lt;- predict(qda_model, newdata = grid)\n\nggplot() +\n    geom_point(data = test_data, aes(x = x1, y = x2, color = y), size = 3)+\n    geom_tile(data = grid, aes(x = x1, y = x2, fill = pred), alpha = 0.3) +\n    labs(title = \"Decision boundary - QDA\", x = \"Variable 1\", y = \"Variable 2\") +\n    theme_minimal()"
  },
  {
    "objectID": "lda_qda.html#practical-2",
    "href": "lda_qda.html#practical-2",
    "title": "4  Discriminant analysis",
    "section": "4.6 Practical 2",
    "text": "4.6 Practical 2\n\nlibrary(class)\nlibrary(caret)\nlibrary(FNN)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(gridExtra)\n\n\nImport the 500 images from the file numbers_train.txt.\n\n\ndataTrain &lt;- read.table(\"./data/numbers_train.txt\", header=TRUE)\n\n\nVisualize the first 16 images.\n\n\n# Function to convert a vector into a matrix 16 * 16\nvector_to_matrix &lt;- function(vec) {\n  matrix(as.numeric(vec), nrow = 16, ncol = 16, byrow = TRUE)\n}\n\n\nplots &lt;- list()\nfor (i in 1:16) {\n  img_matrix &lt;- vector_to_matrix(dataTrain[i, -1])  # Exclude the first column (label)\n  img_df &lt;- expand.grid(x = 1:16, y = 1:16)\n  img_df$intensity &lt;- as.vector(t(img_matrix))\n  \n plots[[i]] =  ggplot(img_df, aes(x, 17-y, fill = intensity)) +\n    geom_tile(show.legend = FALSE) +\n    scale_fill_gradient(low = \"black\", high = \"white\") +\n    coord_fixed() +\n    theme_void() +\n    ggtitle(paste(\"Image\", i))\n}\n\ngrid.arrange(grobs = plots, ncol = 4)\n\n\n\n\n\nPredict with the lda method the classes of the 500 images of the training set and calculate the learning error rate.\n\n\n# Separate explanatory variables (pixels) and target variable (class)\nXtrain &lt;- as.matrix(dataTrain[,-1])\nYtrain &lt;- as.factor(dataTrain[,1])\ntrain_data &lt;- data.frame(Ytrain, Xtrain)\n\n# Fit LDA on training set\nlda_model &lt;- train(Ytrain ~ ., \n                   data = train_data, \n                   method = \"lda\",\n                   trControl = trainControl(method = \"none\")) # No cross-validation\n\n# Predict on training set\ny_pred &lt;- predict(lda_model, Xtrain)\n\n# Calculate error rate\nerror_rate &lt;- mean(y_pred != Ytrain)\n\n\nImport the 500 images from the file numbers_test.txt.\n\n\ndataTest &lt;- read.table(\"./data/numbers_test.txt\", header=TRUE)\n\n\nNow predict the classes of the 500 images of the test set and calculate the test error rate.\n\n\n# Separate the explanatory variables (pixels) and the target variable (class)\nXtest &lt;- as.matrix(dataTest[,-1])\nYtest &lt;- as.factor(dataTest[,1])\n\n# Predict on the test set\ny_pred &lt;- predict(lda_model, Xtest)\n\ny_pred\n\n  [1] 9 6 3 2 6 0 0 0 6 9 6 7 0 8 3 4 1 0 9 6 2 2 8 4 6 2 0 5 0 3 7 0 7 0 7 9 0\n [38] 7 0 7 0 2 1 0 7 1 0 4 9 0 8 5 2 0 0 6 5 9 8 0 9 0 0 4 0 9 1 2 2 1 8 3 7 2\n [75] 0 9 0 1 2 1 2 0 7 8 0 6 4 8 2 0 9 0 4 8 2 0 7 0 0 9 0 8 7 3 0 7 2 5 7 4 0\n[112] 3 9 9 7 0 3 9 9 6 5 0 3 0 6 8 6 6 1 4 8 3 0 1 6 7 0 0 7 7 9 6 6 2 2 0 2 4\n[149] 0 9 6 0 0 4 8 8 1 0 1 0 6 7 0 6 9 2 9 0 3 4 4 0 9 0 9 0 8 5 0 4 1 0 3 0 8\n[186] 0 6 4 1 1 1 8 1 0 3 5 7 2 3 0 9 7 8 0 5 0 1 8 0 0 1 3 5 0 6 0 6 8 0 9 0 9\n[223] 8 6 3 2 1 8 0 3 0 6 4 1 2 1 6 9 1 0 6 5 3 1 6 4 2 5 1 0 1 8 4 8 9 4 2 0 4\n[260] 3 4 7 7 0 8 8 0 9 9 7 9 6 0 1 2 3 6 6 6 3 3 8 0 2 6 8 6 0 1 7 9 8 7 3 7 9\n[297] 9 6 6 7 5 9 9 8 9 7 3 6 8 2 7 3 2 8 1 0 1 1 1 6 7 6 8 5 0 9 4 8 1 8 6 8 6\n[334] 0 8 7 0 0 2 9 4 1 3 1 6 4 1 9 1 7 0 3 9 4 9 0 0 0 1 1 0 0 1 8 1 0 4 0 2 1\n[371] 0 0 1 7 1 0 0 4 0 1 3 9 8 1 0 1 0 9 1 0 1 0 1 1 0 8 0 1 1 0 1 6 3 1 0 0 7\n[408] 6 1 0 0 2 1 1 0 0 3 9 1 8 0 1 0 6 0 0 0 3 1 0 0 5 7 8 9 1 6 0 8 9 1 0 8 0\n[445] 0 9 0 8 8 7 1 1 7 0 2 8 8 7 0 4 9 5 0 2 6 8 9 9 6 7 3 0 5 7 2 0 8 7 9 4 7\n[482] 1 9 7 8 0 1 9 6 8 2 4 0 2 4 8 4 8 8 8\nLevels: 0 1 2 3 4 5 6 7 8 9\n\n# Calculate the error rate\nerror_rate &lt;- mean(y_pred != Ytest)\n\n# Display the result\ncat(\"Training error rate:\", error_rate * 100, \"%\\n\")\n\nTraining error rate: 25.8 %"
  },
  {
    "objectID": "unsup.html#partition",
    "href": "unsup.html#partition",
    "title": "5  Unsupervised classification",
    "section": "5.1 Partition",
    "text": "5.1 Partition\n\n\n\n\n\n\nDefinition\n\n\n\nA partition \\(\\mathcal{P}\\) into \\(K\\) classes of individuals is a set of non-empty classes, two by two disjoint and whose union is the set of individuals:\n\\[\\begin{align*}\nC_k &\\ne \\emptyset, \\quad \\forall k \\in \\{1, \\dots, K\\} \\\\\nC_k \\cap C_{k'} &= \\emptyset, \\quad \\forall k, k' \\in \\{1, \\dots, K\\} \\\\\nC_1 \\cup \\cdots \\cup C_K &= \\Omega\n\\end{align*}\\]\n\n\nFor the example \\(\\Omega = \\{Brigitte, Marie, Vincent, Alex, Manue, Fred\\}\\), I proposed a partition \\(\\mathcal{P}^3 = \\{C1,C2,C3\\}\\) into 3 classes of the 6 individuals below.\n\n\n\n\n\n\n\n\n\\(C1 = \\{Marie, Brigitte\\}\\)\n\\(C2 = \\{Alex, Vincent\\}\\)\n\\(C3 = \\{Manue, Fred\\}\\)"
  },
  {
    "objectID": "unsup.html#hierachy",
    "href": "unsup.html#hierachy",
    "title": "5  Unsupervised classification",
    "section": "5.2 Hierachy",
    "text": "5.2 Hierachy\n\n\n\n\n\n\nDefinition\n\n\n\nA hierarchy H of a set of parts \\(\\mathcal{X} = \\{x_1,...,x_n\\}\\) satisfying:\n\n\\(\\forall i \\in [1,n], \\{x_i\\} \\in H\\)\n\\(\\mathcal{X} \\in \\mathcal{H}\\)\n\\(\\forall A, B \\in \\mathcal{H}, A \\cap B = \\emptyset \\: or A \\subset B \\: or B \\subset A\\)\n\nA dendrogram (or hierarchical tree) is the graphical representation of an indexed hierarchy and the function h measures the height of the classes in this dendrogram.\n\n\n\\(\\mathcal{H} = \\{ \\{Brigitte\\}, \\{Marie\\}, \\{Vincent\\}, \\{Alex\\}, \\{Manue\\}, \\{Fred\\},\\)\n\\(\\{Alex,Fred\\},\\{Brigitte,Vincent\\},\\{Brigitte,Vincent,Manue\\},\\)\n\\(\\{Brigitte, Marie, Vincent, Alex, Manue, Fred\\} \\}\\)\nBy defining a cut level, we will obtain a partition.\n\\(\\mathcal{P} = \\{ \\{Marie\\},\\{Alex,Fred\\}, \\{Manue\\}, \\{Brigitte\\} ,\\{Vincent\\} \\}\\)\n\\(\\mathcal{P} = \\{ \\{Marie\\},\\{Alex,Fred\\}, \\{Manue\\}, \\{Brigitte,Vincent\\} \\}\\)\n\\(\\mathcal{P} = \\{ \\{Marie\\},\\{Alex,Fred\\}, \\{Manue,Brigitte,Vincent\\} \\}\\)\n\\(\\mathcal{P} = \\{ \\{Marie, Alex,Fred\\}, \\{Manue,Brigitte,Vincent\\} \\}\\)\n\n\n\n\n\n\nDendrogram"
  },
  {
    "objectID": "unsup.html#how-to-measure-the-distance-between-individuals",
    "href": "unsup.html#how-to-measure-the-distance-between-individuals",
    "title": "5  Unsupervised classification",
    "section": "5.3 How to measure the distance between individuals ?",
    "text": "5.3 How to measure the distance between individuals ?\nClustering methods require the ability to quantify the dissimilarity between the observations.\n\n5.3.1 Binary data\nFor binary data (i.e. vectors composed of 0 and 1), we construct the cross-table between two individuals \\(i\\) and \\(i′\\) :\n\n\n\\(I_1\\) = {1,0,0,1,1,1,0,0}\n\\(I_2\\) = {0,1,0,1,1,1,1,0}\n\n\n\n\n\n\n1\n0\nindividual i’\n\n\n\n\nindividual i\n1\na\nb\n\n\n\n\n0\nc\nd\n\n\n\n\n\n\nThere are then several normalized similarity indices (\\(s_{max} = 1\\)):\n\n\nJaccard \\(\\frac{a}{a+b+c}\\)\nRussel and Rao \\(\\frac{a}{2a+b+c+d}\\)\n\nDice or Czekanowski \\(\\frac{2a}{2a+b+c}\\)\nOchiai \\(\\frac{a}{\\sqrt{a+b} + \\sqrt{a+c}}\\)\n\n\nA dissimilarity index:\n\\[\nd(i,i') = s_{max} - s(i,i')\n\\]\n\n\n5.3.2 Quantitative data\nFor quantitative data x and y of \\(\\mathcal{R}^P\\):\n\nsimple Euclidean distance :\n\\[d^2(x,y)=\\sum\\nolimits_{j=1}^{p}(x_j - y_j)^2\\]\nnormalized Euclidean distance :\n\\[d^2(x,y)=\\sum\\nolimits_{j=1}^{p}\\frac{1}{s^2_j}(x_j - y_j)^2\\] where \\(s^2_j = \\frac{1}{n}\\sum\\nolimits_{i=1}^{n}(x_ij-x^{-j})^2\\) and \\(x^{-j} = \\frac{1}{n}\\sum\\nolimits_{i=1}^{n}x_{ij}\\)\ncity-block or Manhattan distance:\n\\[d(x,y) = \\sum\\nolimits_{j}^{}|x_j-y_j|\\]\nChebyshev or max distance :\n\\[d(x,y) = max_{j}|x_j-y_j|\\]\n\n\n\n\n\n\n\nNote\n\n\n\nIn general, we use the simple Euclidean distance when all the variables have the same measurement scale\nIn the case of measurement scales that are too different, it is preferable to use the normalized Euclidean distance in order to give the same importance to all the variables."
  },
  {
    "objectID": "unsup.html#how-to-measure-the-distance-between-classes",
    "href": "unsup.html#how-to-measure-the-distance-between-classes",
    "title": "5  Unsupervised classification",
    "section": "5.4 How to measure the distance between classes ?",
    "text": "5.4 How to measure the distance between classes ?\n\n5.4.1 Linkage function\n\n\nMinimum link\n\\[\nD(C_k,C_{k'}) = \\min_{x \\in C_k,x' \\in C_k'} d(x,x')\n\\]\n\n\n\n\n\n\nMinimal spanning tree:\n\nClasses with very different diameters\nChaining effect: tendency to aggregate rather than create new classes\nSensitivity to noisy individuals\n\n\n\nMiximal link\n\\[\nD(C_k,C_{k'}) = \\max_{x \\in C_k,x' \\in C_k'} d(x,x')\n\\] \n\nCreates compact classes (diameter control): this fusion generates the smallest increase in diameters:\n\nSensitivity to noisy individuals\n\n\n\n\n\n\nThe average link\n\\[\nD(C_k,C_{k'}) = \\frac {1}{|C_k||C_{k'}|}\\sum_{x \\in C_k}^{}\\sum_{x' \\in C_{k'}}^{}d(x,x')\n\\]\n\n\n\n\n\n\nTrade-off between minimal and maximal links : good balance between class separation and class diameter diameter\nTendency to produce classes of close variance\n\n\nThe Ward’s link\n\\[\nD(C_k,C_{k'}) = \\frac {|C_k|+|C_{k'}|}{|C_k||C_{k'}|}d(\\mu_k,\\mu_{k'})^2\n\\]\n\n\n\n\n\nwhere \\(\\mu_k,\\mu_{k'}\\) are gravity centers of \\(C_k,C_{k'}\\)\n\nTendency to build classes of the same size for a given level of hierarchy.\nGroups classes with close barycenters\nBreaks the chain effect of the minimum link"
  },
  {
    "objectID": "unsup.html#how-to-evaluate-the-quality-of-a-partition",
    "href": "unsup.html#how-to-evaluate-the-quality-of-a-partition",
    "title": "5  Unsupervised classification",
    "section": "5.5 How to evaluate the quality of a partition ?",
    "text": "5.5 How to evaluate the quality of a partition ?\nA good partition into K classes has classes:\n\nhomogeneous : individuals in the same class are similar,\nseparate : individuals from two different classes are not similar.\n\n\n\nThe cohesion of the classes of a partition can be measured by the largest diameter.\n\n\n\n\n\n\nThe separation of the classes of a partition can be measured by the smallest minimum link.\n\n\n\n\n\n\n\nWe consider a partition \\(\\mathcal{P}_K = {C_1,··· ,C_K}\\) in \\(K\\) classes. We assume here that the data are quantitative and that the weight of the individuals is \\(\\frac{1}{n}\\).\nWe note \\(\\mu\\) the center of gravity of the point cloud\n\\[\\mu = \\frac{1}{n}\\sum\\nolimits_{i=1}^{n} x_i\\]\nand for each class k, \\(\\mu_k\\) the center of gravity of the class \\(k\\)\n\\[\\mu_k = \\frac{1}{|C_k|}\\sum_{i \\in C_k}^{} x_i, for\\ all\\ k \\in K\\]\n\n\n\n\n\n\nNote\n\n\n\nTotal inertia (independent of the partition) = total variance\n\\[I_{Tot} = \\sum_{i = 1}^{n} d(\\mu,x_i)^2 = I_{Inter} + I_{Intra}\\]\nInter-class inertia = variance of the class centers\n\\[I_{Inter} = \\sum_{k = 1}^{K} |C_k| d(\\mu,\\mu_k)^2\\]\nIntra-class inertia = variance of points in the same class\n\\[I_{Intra} = \\sum_{k = 1}^{K}\\sum_{i \\in C_k}^{} d(\\mu,x_i)^2\\]\n\n\nTo obtain a good partitioning, it is therefore appropriate to both :\n\nminimize the intra-class inertia to obtain the most homogeneous clusters possible\nmaximize the inter-class inertia to obtain well-differentiated subsets\n\n\n\nInternal metric (practical situation - unknown truth):\n\nsilhouette coefficient\nR-Square (RSQ) and semi-partial R-Square (SPRSQ)\n\n\nExternal metric (specific method if we know the truth):\n\npurity\nnormalized mutual information\n\n\n\nAn example of internal metric : coefficient silhouette\nWe assume that we have n points and K clusters. Let \\(x_i\\) be a data such that \\(i \\in C_k\\).\nCohesion = average distance between \\(x_i\\) and the other points of \\(C_k\\)\n\\[a(i) = \\frac {1}{|C_k|-1}\\sum_{j \\in C_k, j \\neq i}d(x_i,x_j)\\]\nSeparation = average distance between \\(x_i\\) and the other points of the closet classes:\n\\[b(i) = \\min_{l \\neq k }\\frac {1}{|C_l|}\\sum_{j \\in C_l}d(x_i,x_j)\\]\nCoefficient silhouette:\n\\[s(i) = \\frac{b(i)-a(i)}{max(a(i),b(i))} \\in [-1,1]\\]\nAn example of internal metric : criteria based on inertia\nLet \\(\\mathscr{P}_K\\) be a partition\n\nR-square:\n\n\\[\nRSQ(\\mathscr{P}_K) = \\frac{I_{Inter}(\\mathscr{P}_K)}{I_{Tot}} = 1- \\frac{I_{Intra}(\\mathscr{P}_K)}{I_{Tot}}\n\\]\n\nSemi-partial R-square:\n\n\\[\nRSQ(\\mathscr{P}_K) = \\frac{I_{Inter}(\\mathscr{P}_K)-I_{Inter}(\\mathscr{P}_{K-1})}{I_{Tot}}\n\\]\nAn example of an external metric : purity\nLet \\(\\mathscr{P}_K^{*} = \\{ C_1^*,..., C_K^* \\}\\) be the true partition of the n points.\nConsider a partition \\(\\mathscr{P}_K = \\{ C_1,..., C_K \\}\\).\n\\[\nPurity(\\mathscr{P}_K) =\\frac{1}{n} \\sum_{k=1}^K \\max_{l \\in \\{1,...,K^*\\}} |C_l^{*} \\cap C_k|\n\\]"
  },
  {
    "objectID": "had.html#principle",
    "href": "had.html#principle",
    "title": "6  Hierachical ascending classification",
    "section": "6.1 Principle",
    "text": "6.1 Principle\n\n6.1.1 First strategy: Agglomerative Hierarchical Clustering\n\nStart from the bottom of the dendrogram (singletons),\nAdd the closest parts two by two until you get a single class\n\n\n\n\n\n\n\nSource: Janssen, Walther, and Lüdeke (2012)\n\n\n\n\n\n\n\nSource: Data analysis MOOC of Francois Husson\n\n\n\n\n\n\n\nWhere to cut the dendogram?\n\n\n\nRule of thumb\n\nSelection of a cut when there is a significant jump in the index by visual inspection of the tree. This jump reflects the sudden passage from a certain homogeneity of classes to much less homogeneous classes.\n\n\n\n\n\n6.1.2 Second strategy: Divide the hierarchical clustering\n\nStart from the top of the dendrogram (one unique class),\nSuccessive divisions until you get classes reduced to singlets."
  },
  {
    "objectID": "had.html#weaknesses-and-strengths",
    "href": "had.html#weaknesses-and-strengths",
    "title": "6  Hierachical ascending classification",
    "section": "6.2 Weaknesses and strengths",
    "text": "6.2 Weaknesses and strengths\nAdvantages\n\nSimple considerations of distances and similarities\nNo assumption on the number of classes\nCan correspond to significant taxonomies\n\nDisadvantages\n\nChoice of the dendogram cutoff.\nThe partition obtained at a step depends on that of the previous step.\nOnce a decision is made to combine classes, it cannot be undone.\nToo slow for large datasets."
  },
  {
    "objectID": "had.html#practical",
    "href": "had.html#practical",
    "title": "6  Hierachical ascending classification",
    "section": "6.3 Practical",
    "text": "6.3 Practical\n\n6.3.1 Example 1\nWe consider the following data table where 4 individuals (here points) A,B,C and D are described on two variables (X1 and X2):\n\n\n\n\n\n\nX1\nX2\n\n\n\n\nA\n5\n4\n\n\nB\n4\n5\n\n\nC\n1\n-2\n\n\nD\n0\n-3\n\n\n\n\n\n\n\n\n\n\n\n\n6.3.1.1 Construct the dendrogram using maximal link\n\\(\\mathcal{P}^{\\{0\\}} = \\{\\{A\\},\\{B\\},\\{C\\},\\{D\\}\\}\\)\n\nStep 1:\n\n\n\n\\(d(A,B)\\) = 1.4142136\n\\(d(A,C)\\) = 7.2111026\n\\(d(A,D)\\) = 8.6023253\n\\(d(B,C)\\) = 7.6157731\n\\(d(B,D)\\) = 8.9442719\n\\(d(C,D)\\) = 1.4142136\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\nA\n0\n\n\n\n\n\nB\n1.4142136\n0\n\n\n\n\nC\n7.2111026\n7.6157731\n0\n\n\n\nD\n8.6023253\n8.9442719\n1.4142136\n0\n\n\n\n\n\n=&gt; \\(\\mathcal{P}^{\\{1\\}} = \\{\\{A,B\\},\\{C\\},\\{D\\}\\}\\)\nWe can choose: minimal link, maximum link, average link, or Ward’s link. In this case, I selected maximum link:\n\\[\\begin{align}\n\nD(C_k,C_{k'}) = \\max_{x \\in C_k ,\\: x' \\in C_{k'}} d(x,x')\n\n\\end{align}\\]\n\n\n\nStep 2:\n\n\\(D(\\{A,B\\},\\{C\\}) \\\\\\)\n= \\(max(d(A,C),d(B,C))\\)\n= \\(max(7.21,7.61) = {\\bf 7.61}\\)\n\\(D(\\{A,B\\},\\{D\\}) \\\\\\)\n= \\(max(d(A,D),d(B,D))\\)\n= \\(max(8.6,8.9) = {\\bf 8.9}\\)\n\\(D(\\{C\\},\\{D\\}) = d(C,D) = 1.41\\)\n\\(D(\\{A\\},\\{B\\}) = d(A,B) = 1.41\\)\n=&gt; \\(\\mathcal{P}^{\\{2\\}} = \\{\\{A,B\\},\\{C,D\\}\\}\\)\n\n\nStep 3:\n\n\\(D(\\{A,B\\},\\{C,D\\})\\)\n= \\(max(d(A,C),d(A,D),d(B,C),d(B,D))\\)\n= 8.9\n=&gt; \\(\\mathcal{P}^{\\{3\\}} = \\{A,B,C,D\\}\\)\n\n\n\n\nCode\nd &lt;- stats::dist(X) # compute the Euclidean distances between points\n\ntreeC &lt;- hclust(d, method=\"complete\")\n\ntreeC$height\n\n\n[1] 1.414214 1.414214 8.944272\n\n\nCode\nggdendrogram(treeC, rotate = FALSE, size = 2)+\n  scale_y_continuous(breaks = seq(0,10.5,by=1.5))+\n  labs(title = \"Maximal link's HAC\")\n\n\n\n\n\nCode\ncutree(treeC, k=2 )\n\n\nA B C D \n1 1 2 2 \n\n\n\n\n6.3.1.2 Construct the dendrogram by Ward’s link\n\nStep 1:\n\n\n\n\\(d(A,B)\\) = 1.4142136\n\\(d(A,C)\\) = 7.2111026\n\\(d(A,D)\\) = 8.6023253\n\\(d(B,C)\\) = 7.6157731\n\\(d(B,D)\\) = 8.9442719\n\\(d(C,D)\\) = 1.4142136\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\nA\n0\n\n\n\n\n\nB\n1.4142136\n0\n\n\n\n\nC\n7.2111026\n7.6157731\n0\n\n\n\nD\n8.6023253\n8.9442719\n1.4142136\n0\n\n\n\n\n\n=&gt; \\(\\mathcal{P}^{\\{1\\}} = \\{\\{A,B\\},\\{C,D\\}\\}\\)\n\nStep 2:\n\nWard’s link:\n\\[\nD(C_k,C_{k'}) = \\frac {|C_k||C_{k'}|}{|C_k|+|C_{k'}|}d(\\mu_k,\\mu_{k'})^2\n\\]\n\\[\\begin{cases}\n\\mu_1 = \\frac{A+B}{2} = (4.5,4.5)\\\\\n\n\\mu_2 = \\frac{C+D}{2} = (0.5,-2.5)\n\\end{cases}\\]\n\\[\\begin{align}\n\nD(C_k,C_{k'}) &= \\frac {|C_k||C_{k'}|}{|C_k|+|C_{k'}|}d(\\mu_k,\\mu_{k'})^2 \\\\\n\n&= \\frac{2 \\times 2}{2+2}d(\\mu_k,\\mu_{k'})^2 \\\\\n\n&= (4.5-0.5)^2 + ((4.5+2.5)^2) \\\\\n\n&= 4^2 + 7^2 \\\\\n\n&= 65\n\n\\end{align}\\]\n\n\nCode\ntreeW &lt;- hclust(d, method=\"ward.D2\")\n\ntreeW$height\n\n\n[1]  1.414214  1.414214 11.401754\n\n\nCode\nggdendrogram(treeW, rotate = FALSE, size = 2)+\n  labs(title = \"Ward's HAC\")\n\n\n\n\n\n\n\n\n6.3.2 Example 2\n\nlibrary(ggplot2)\nlibrary(cluster)\nlibrary(dendextend)\nlibrary(factoextra)\nlibrary(ggdendro)\n\n\nStep 1: Data preparation\n\nset.seed(123)\ndata &lt;- data.frame(\n  x = c(rnorm(50, mean = 2, sd = 0.5), rnorm(50, mean = 5, sd = 0.5)),\n  y = c(rnorm(50, mean = 3, sd = 0.5), rnorm(50, mean = 6, sd = 0.5))\n  )\n\nggplot(data, aes(x = x, y = y)) +\n  geom_point(color = 'blue') +\n  theme_minimal() +\n  ggtitle(\"Initial data\")\n\n\n\n\n\n\nStep 2: HAC\nComputation of the distance matrix\n\ndistance_matrix &lt;- dist(data, method = \"euclidean\")\n\nHierarchical ascending classification\n\ncah &lt;- hclust(distance_matrix, method = \"ward.D2\")\n\n# Conversion into format ggplot2\ndendro_data &lt;- ggdendro::dendro_data(cah)\n\n# Extraction of the labels of the leaves\nlabel_data &lt;- dendro_data$labels\n\n# Display of the basic dendogram with ggplot2\nggplot() +\n  geom_segment(data = dendro_data$segments, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_text(data = label_data, aes(x = x, y = y, label = label),\n            hjust = 2, angle = 90, size = 2) +\n  labs(title = \"HAC dendrogram\", x = \"\", y = \"Height\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), panel.grid = element_blank())\n\n\n\n\n\n\nStep 3: Visualization of the results with ggplot2\n\n# Cutting in clusters\n\nk &lt;- 2 # Number of desired clusters\nclusters &lt;- cutree(cah, k = k)\n\ndata$cluster &lt;- as.factor(clusters)\n\n# clusters visualization\nggplot(data, aes(x = x, y = y, color = cluster)) +\ngeom_point(size = 3) +\ntheme_minimal() +\nggtitle(paste(\"Classification in\", k, \"clusters\"))\n\n\n\n\n\n\n\n\nJanssen, Peter, Carsten Walther, and M. Lüdeke. 2012. Cluster Analysis to Understand Socio-Ecological Systems: A Guideline. Vol. 126."
  },
  {
    "objectID": "kmean.html#the-algorithm",
    "href": "kmean.html#the-algorithm",
    "title": "7  k-means",
    "section": "7.1 The algorithm",
    "text": "7.1 The algorithm\nOptimal partition:\n\nChoose from all the partitions into K classes the one with the greatest inter inertia.\nProblem : number of partitions into K classes of the n individuals \\(\\sim \\frac{K^n}{K!}.K!\\)\n\n=&gt; Complete enumeration impossible.\nLocally optimal partition - Heuristic of the type :\n\nWe start from a feasible solution, i.e. a partition \\(\\mathscr{P}_K^0\\)\nAt step t + 1, we look for a partition \\(\\mathscr{P}_{K}^{t+1} = g(\\mathscr{P}_{K}^{t})\\) such that inertia_inter(\\(\\mathscr{P}_{K}^{t+1}\\)) inertia_inter(\\(\\mathscr{P}_{K}^{t}\\))\nStop when no individual changes class between two iterations\n\n=&gt; Method for partitioning K-means.\nInitialization step"
  },
  {
    "objectID": "ml_theory.html#principle-of-machine-learning",
    "href": "ml_theory.html#principle-of-machine-learning",
    "title": "Machine Learning theory",
    "section": "Principle of machine learning",
    "text": "Principle of machine learning"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Janssen, Peter, Carsten Walther, and M. Lüdeke. 2012. Cluster\nAnalysis to Understand Socio-Ecological Systems: A Guideline. Vol.\n126.\n\n\nSk, Singh. 2020. “The Hold-Up of the Century: Neural Networks Are\nComing from Cognitive Science and Not Machine Learning. Perspectives to\nAvoid a New Dark Age of Artificial Intelligence.” Trends in\nArtificial Intelligence 4 (1). https://doi.org/10.36959/643/306."
  }
]