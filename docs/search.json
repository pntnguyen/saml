[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics and Machine Learning",
    "section": "",
    "text": "Preface\nThe content of this book is based on the materials of the Spring School on Statistic and Machine Learning, which was held by the Vietnam Institute for Advanced Study in Mathematics.\n\n\n\nMaterials:\n\nDay 1:\n\nClassification\nComputational Statistics"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\nCode\n1 + 1\n\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "1  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n\nCode\n1 + 1\n\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Sk, Singh. 2020. “The Hold-Up of the Century: Neural Networks Are\nComing from Cognitive Science and Not Machine Learning. Perspectives to\nAvoid a New Dark Age of Artificial Intelligence.” Trends in\nArtificial Intelligence 4 (1). https://doi.org/10.36959/643/306."
  },
  {
    "objectID": "classification.html#introduction-to-learning",
    "href": "classification.html#introduction-to-learning",
    "title": "Statistical Classification",
    "section": "Introduction to learning",
    "text": "Introduction to learning\n\nWhat is AI?\nArtificial Intelligent (AI) is a field of computer science and mathematics that brings together a set of algorithmic techniques and theories for creating machines that mimic human intelligence.\nQuestion:\n\nHow do Artificial Intelligent (AI) and machine learning really work?\nHow do they learn from our behaviors, preferences, and interactions ?\n\nThe aim of AI is to simulate human intelligence, and in particular to learn a wide range of tasks. There are two possible ways of learning :\n\nRote learning: explicitly memorizing all possible examples to play them back\nGeneralizing learning: extract implicit rules from a large number of examples to reapply them to new situations never encountered before\n\nRote learning is relatively easy for a machine, as long as the examples are available. On the other hand, learning by generalization is difficult, as it requires the extraction of rules that are not explicitly mentioned in the examples\n\n\n\n\n\n\nRelationship between AI, ML, Neural Networks, and Deep Learning (Sk 2020)\n\n\n\nWhat is machine learning?\nMachine learning is a sub-domain of AI, which involves learning from experience or from a database of implicit rules to answer to a given problem. This field focuses on the statistical analysis of training data. This field focuses on the statistical analysis of training data."
  },
  {
    "objectID": "classification.html#supervised-and-unsupervised-learning",
    "href": "classification.html#supervised-and-unsupervised-learning",
    "title": "Statistical Classification",
    "section": "Supervised and unsupervised learning",
    "text": "Supervised and unsupervised learning"
  },
  {
    "objectID": "classification.html#introduction-to-classification",
    "href": "classification.html#introduction-to-classification",
    "title": "Statistical Classification",
    "section": "Introduction to classification",
    "text": "Introduction to classification\n\n\n\n\nSk, Singh. 2020. “The Hold-Up of the Century: Neural Networks Are Coming from Cognitive Science and Not Machine Learning. Perspectives to Avoid a New Dark Age of Artificial Intelligence.” Trends in Artificial Intelligence 4 (1). https://doi.org/10.36959/643/306."
  },
  {
    "objectID": "intro.html#introduction-to-learning",
    "href": "intro.html#introduction-to-learning",
    "title": "Introduction",
    "section": "Introduction to learning",
    "text": "Introduction to learning\n\nWhat is AI?\nArtificial Intelligent (AI) is a field of computer science and mathematics that brings together a set of algorithmic techniques and theories for creating machines that mimic human intelligence.\nQuestion:\n\nHow do Artificial Intelligent (AI) and machine learning really work?\nHow do they learn from our behaviors, preferences, and interactions ?\n\nThe aim of AI is to simulate human intelligence, and in particular to learn a wide range of tasks. There are two possible ways of learning :\n\nRote learning: explicitly memorizing all possible examples to play them back\nGeneralizing learning: extract implicit rules from a large number of examples to reapply them to new situations never encountered before\n\nRote learning is relatively easy for a machine, as long as the examples are available. On the other hand, learning by generalization is difficult, as it requires the extraction of rules that are not explicitly mentioned in the examples\n\n\n\n\n\n\nRelationship between AI, ML, Neural Networks, and Deep Learning (Sk 2020)\n\n\n\nWhat is machine learning?\nMachine learning is a sub-domain of AI, which involves learning from experience or from a database of implicit rules to answer to a given problem. This field focuses on the statistical analysis of training data. This field focuses on the statistical analysis of training data.\nGenerally speaking, machine learning algorithms are divided into several phases:\n\nTraining phase (or learning phase):\n\nThe chosen model is subjected to a large number of significant examples\nThe system then seeks to learn implicit rules based on this data (called training data)\n\nInference phase:\n\nThe trained model can be used on new inputs\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe training phase generally precedes the use of the model, although some systems continue to learn indefinitely if they have feedback on the results (this is called on-line learning).\nInputs provided during the inference phase can be processed even if they were not seen by the model during the learning phase.\n\n\n\n\n\nType of machine learning\n\nMachine learning uses different types of learning, with supervised learning and unsupervised learning playing a prominent role.\nDeep learning is a set of techniques that use neural networks to solve complex problems.\nReinforcement learning consists in learning by interacting with the agent’s environment."
  },
  {
    "objectID": "intro.html#supervised-and-unsupervised-learning",
    "href": "intro.html#supervised-and-unsupervised-learning",
    "title": "Introduction",
    "section": "Supervised and unsupervised learning",
    "text": "Supervised and unsupervised learning\n\nSupervised learning\nWe have the labeled training data \\((x_{i},y_{i})_{i=1,···,n}\\), the n inputs \\(x_{i}\\) and the associated target outputs \\(y_{i}\\). The aim is to train the chosen model so that it can correctly predict the output for unlabeled inputs.\n\n\n\n\n\nSupervised learning is generally used for regression or classification:\n\nRegression is used when the output to be predicted can take continuous values\nClassification is the task of choosing a class (value) from all those possible.\n\nClassic supervised learning algorithms include linear regression, nearest neighbor algorithms, discriminant factor analysis, logistic regression, neural networks, decision trees, random forests and support vector machines.\n\n\nUnsupervised learning\nWe therefore have input data for which we don’t know the associated output. The data set is therefore \\((x_{i})_{i=1,···,n}\\) and the aim of the system is to identify features common to the training data.\nUnsupervised learning is mainly composed of clustering algorithms. These algorithms seek to separate the input data into a given number of groups. Each element in the group must have characteristics close to those of elements in the same group, but relatively distant from those of other groups.\nThe most common unsupervised learning algorithms are the k-means algorithm, hierarchical ascending classification, principal component analysis, DBSCAN, singular value decomposition and some neural networks.\n\n\n\n\nSk, Singh. 2020. “The Hold-Up of the Century: Neural Networks Are Coming from Cognitive Science and Not Machine Learning. Perspectives to Avoid a New Dark Age of Artificial Intelligence.” Trends in Artificial Intelligence 4 (1). https://doi.org/10.36959/643/306."
  },
  {
    "objectID": "intro.html#introduction-to-machine-learning",
    "href": "intro.html#introduction-to-machine-learning",
    "title": "Introduction",
    "section": "Introduction to machine learning",
    "text": "Introduction to machine learning\n\nWhat is AI?\nArtificial Intelligent (AI) is a field of computer science and mathematics that brings together a set of algorithmic techniques and theories for creating machines that mimic human intelligence.\nQuestion:\n\nHow do Artificial Intelligent (AI) and machine learning really work?\nHow do they learn from our behaviors, preferences, and interactions ?\n\nThe aim of AI is to simulate human intelligence, and in particular to learn a wide range of tasks. There are two possible ways of learning :\n\nRote learning: explicitly memorizing all possible examples to play them back\nGeneralizing learning: extract implicit rules from a large number of examples to reapply them to new situations never encountered before\n\nRote learning is relatively easy for a machine, as long as the examples are available. On the other hand, learning by generalization is difficult, as it requires the extraction of rules that are not explicitly mentioned in the examples\n\n\n\n\n\n\nRelationship between AI, ML, Neural Networks, and Deep Learning (Sk 2020)\n\n\n\nWhat is machine learning?\nMachine learning is a sub-domain of AI, which involves learning from experience or from a database of implicit rules to answer to a given problem. This field focuses on the statistical analysis of training data. This field focuses on the statistical analysis of training data.\nGenerally speaking, machine learning algorithms are divided into several phases:\n\nTraining phase (or learning phase):\n\nThe chosen model is subjected to a large number of significant examples\nThe system then seeks to learn implicit rules based on this data (called training data)\n\nInference phase:\n\nThe trained model can be used on new inputs\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe training phase generally precedes the use of the model, although some systems continue to learn indefinitely if they have feedback on the results (this is called on-line learning).\nInputs provided during the inference phase can be processed even if they were not seen by the model during the learning phase.\n\n\n\n\n\nType of machine learning\n\nMachine learning uses different types of learning, with supervised learning and unsupervised learning playing a prominent role.\nDeep learning is a set of techniques that use neural networks to solve complex problems.\nReinforcement learning consists in learning by interacting with the agent’s environment."
  },
  {
    "objectID": "intro_classification.html#introduction-to-classification",
    "href": "intro_classification.html#introduction-to-classification",
    "title": "Classification",
    "section": "Introduction to classification",
    "text": "Introduction to classification\nThe aim of classification is to group (partition, segment) \\(n\\) observations into a number of homogeneous groups or classes. There are two main types of classification :\n\nSupervised classification, often referred to simply as classification\nUnsupervised classification, sometimes called partitioning, segmentation, or clustering."
  },
  {
    "objectID": "intro_classification.html#supervised-classification",
    "href": "intro_classification.html#supervised-classification",
    "title": "Classification",
    "section": "Supervised classification",
    "text": "Supervised classification\nIn supervised classification,\n\nWe already know how many groups exist in the population\nWe know the group to which each observation in the population belongs\nWe want to classify the observations in the right groups based on different variables\n\nWe can then use a classification rule to predict the groups to which new observations belong.\nSome examples of applications:\n\nRecognize hand written numbers\nIdentify the type of cancer patients has\n\nThere are several families of supervised classification methods. The most common are nearest neighbor method, discriminant factor analysis, classification trees, logistic regression, naive Bayesian, neural networks, support vector machines.\nWe have the data with n individuals described by their values of X and Y.\n\\[\\begin{aligned}\nX_n =\n\\begin{bmatrix}\n    x_{11} & x_{12} & x_{13} & \\dots  & x_{1p} \\\\\n    x_{21} & x_{22} & x_{23} & \\dots  & x_{2p} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    x_{n1} & x_{n2} & x_{n3} & \\dots  & x_{np}\n\\end{bmatrix}\n\n\\quad\net\n\\quad\n\nY_n =\n\\begin{bmatrix}\n    y_{1} \\\\\n    y_{2} \\\\\n    \\vdots \\\\\n    y_{n}\n\\end{bmatrix}\n\\end{aligned}\\]\nWe want to predict the class \\(y_0\\) of a new input \\(x_0 = (x_{01},x_{02},...,x_{0p})\\)"
  },
  {
    "objectID": "intro_classification.html#unsupervised-classification",
    "href": "intro_classification.html#unsupervised-classification",
    "title": "Classification",
    "section": "Unsupervised classification",
    "text": "Unsupervised classification\nIn unsupervised classification,\n\nIn general, we don’t know how many groups exist in the population\nWe don’t know the group to which each observation in the population belongs\nWe want to classify observations into homogeneous groups based on different variables\n\nApplication examples:\n\nIn psychology : the determination of personality types present in a group of individuals\nIn text mining : partitioning e-mails or texts according to subject\n\nThere are several families of unsupervised classification methods. The most common are hierarchical classification, k-means method, density-based classification, mixture of normal distributions."
  },
  {
    "objectID": "pred_per.html#confusion-matrix",
    "href": "pred_per.html#confusion-matrix",
    "title": "1  Predictive performance",
    "section": "1.1 Confusion matrix",
    "text": "1.1 Confusion matrix\nThe confusion matrix counts the occurrences of predictions according to the true values.\n\nwhere \\(n_{kℓ}\\) is the number of observations of class \\(k\\) predicted in the class \\(ℓ\\)"
  },
  {
    "objectID": "pred_per.html#empirical-risk",
    "href": "pred_per.html#empirical-risk",
    "title": "1  Predictive performance",
    "section": "1.2 Empirical risk",
    "text": "1.2 Empirical risk\nThe empirical risk (average cost of misclassification) of the g classification rule is\n\\[\nR(g) = \\frac{1}{n} \\sum_{k=1}^{K}\\sum_{l=1}^{K} C_{kl}n_{kl}\n\\] Where\n\\[\nC_{kl} =\n\\begin{cases}\n1 & k = l \\\\\n0 &  k \\neq l\n\\end{cases}\n\\]\nIn the case of a 0-1 cost, we find the empirical error rate\n\\[\nR(g) = \\frac{1}{n} \\sum_{k=1}^{K}\\sum_{l=1 \\: l\\neq k}^{K} n_{kl}\n\\]"
  },
  {
    "objectID": "pred_per.html#predictive-performances-in-binary-classification",
    "href": "pred_per.html#predictive-performances-in-binary-classification",
    "title": "1  Predictive performance",
    "section": "1.3 Predictive performances in binary classification",
    "text": "1.3 Predictive performances in binary classification\n\n1.3.1 Precision/Recall/Specificity\n\n\nConfusion matrix in binary classification\n\n\nTrue positive rate (TPR) is also called sensitivity, recall.\nThe false positive rate (FPR) corresponds to 1 - specificity.\nThe postive predictive value (PPV) is also called precision.\n\n\n\n1.3.2 F1- Score\nIn binary classification, the F1-score depends on:\n\nThe positive predictive value (PPV), also called precision.\nThe true positive rate (TPR), also called sensitivity, recall.\n\n\\[\nF_{1} = \\frac{2PPV \\times TPR}{PPV + TPR} = \\frac{2TP}{2TP + FP + FN}\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\\begin{align}\nF_{1} &= \\frac{2PPV \\times TPR}{PPV + TPR} \\\\\n      &= \\frac{2 \\frac{TP}{TP+FP}\\times\\frac{TP}{TP+FN}}{\\frac{TP}{TP+FP} + \\frac{TP}{TP+FN}} \\\\\n\nNumerator &=  2 \\frac{TP^2}{(TP+FP)(TP+Fn)} \\\\\n\nDenominator &= \\frac{TP(TP+FN)+TP(TP+FP)}{(TP+FP)(TP+FN)} = \\frac{TP(2TP+FN+FP)}{(TP+FP)(TP+FN)} \\\\\n\nF_{1} &= \\frac{2TP^{2}}{(TP+FP)(TP+FN)} \\cdot  \\frac{(TP+FP)(TP+FN)}{TP(2TP+FN+FP)}\\\\\n\n      &= \\frac{2TP^{2}}{TP(2TP+FN+FP)}\\\\\n      \n      &= \\frac{2TP}{2TP+FN+FP}\n\n\\end{align}\\]\n\n\n\nIt measures the classification rule’s ability to correctly predict class 1 entries and not predict 1 of the class 2 entries.\nIn the case where the predictions are no longer binary (multi-class), the F-measure is calculated by making the average of F1 scores for each class.\n\n\n1.3.3 Kappa de Cohen\nIn statistics, the kappa method (kappa) measures agreement between observers during qualitative coding into categories.\n\\[\n\\kappa = \\frac{Pr(a) - Pr(e)}{1 - Pr(e)}\n\\]\nWhere:\n\n\\(Pr(a)\\) is the proportion of agreement between coders\n\\(Pr(e)\\) is the proportion of a random agreement\n\n\n\n\n\n\n\nMarc and Simon are responsible for defining who will be accepted or not at the final exam in a group of 50 students. Each of them checks the copy of each student and notes received or not (YES or NO)\n\n\\[\nP_{a} = \\frac {a+d}{a+b+c+d} = \\frac{20+15}{50}\n\\]\nTo calculate the probability of agreement “at random”, we note :\n\nSimon scored YES to 25 students, or 50% of the cases.\nMarc scored YES in 60%, 30 out of 50 students.\n\n\\[\nP_{YES} = \\frac{a+b}{a+b+c+d} \\times \\frac{a+c}{a+b+c+d} = 0.5 \\times 0.6 = 0.3\n\\]\n\\[\nP_{NO} = \\frac{c+d}{a+b+c+d} \\times \\frac{b+d}{a+b+c+d} = 0.5 \\times 0.4 = 0.2\n\\]\nThe global probability that the teachers agree is:\n\\[\nP_{e} = P_{YES} + P_{NO} = 0.3 + 0.2 = 0.5\n\\]\nKappa’s formula then gives :\n\\[\n\\kappa = \\frac{Pr(a) - Pr(e)}{1 - Pr(e)} = \\frac{0.7 - 0.5}{1 - 0.5} = 0.4\n\\]\n\n\n1.3.4 ROC curve\n\nThe ROC curve is a graphical representation used to evaluate the performance of a binary classification model. It illustrates the trade-off between sensitivity (TPR) and specificity (1 - FPR) on different threshold parameters.\nThe shape of the ROC curve gives an overview of the efficiency of a classification model. A curve that slopes towards the upper left corner indicates a model with high sensitivity and specificity, while a curve closer to the diagonal line suggests a model with low discriminative power.\nThe area under the ROC curve (AUC) is an essential measure of model performance. It is calculated by integrating the area under the ROC curve, providing a single scalar value that summarizes the model’s ability to distinguish between classes. An AUC:\n\nof 0.5 suggests absence of discriminating ability\nfrom 0.7 to 0.8 is considered as acceptable\ngreater than 0.8 indicates good performance\ngreater than 0.9 suggests excellent performance\nof 1 indicates perfect classification\n\n\n\n\n\n\n\n\nTake home message\n\n\n\nA good model is both:\n\nsensitive and specificity. This is measured with the ROC curve and the AUC.\nsensible and accurate. This is measured with the F1-measure."
  },
  {
    "objectID": "pred_per.html#section",
    "href": "pred_per.html#section",
    "title": "1  Predictive performance",
    "section": "1.4 ",
    "text": "1.4"
  }
]